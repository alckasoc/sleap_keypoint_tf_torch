{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Below are links to resources related to this notebook:\n",
        "* [W&B Project](https://wandb.ai/vincenttu/torch_vs_tf_talmo_lab?workspace=user-vincenttu)\n",
        "* [GitHub](https://github.com/alckasoc/sleap_keypoint_tf_torch)\n",
        "\n"
      ],
      "metadata": {
        "id": "8A-NjhkjgVY8"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BeeqrLbdupmE"
      },
      "source": [
        "# Install SLEAP\n",
        "Don't forget to set **Runtime** -> **Change runtime type...** -> **GPU** as the accelerator."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BYxJ2rJOMW8B",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2dcbfa64-0a5f-437a-c826-a2f727162f42"
      },
      "source": [
        "!pip install sleap -qqq\n",
        "!pip install nvidia-ml-py3 -qqq"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m64.4/64.4 MB\u001b[0m \u001b[31m11.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.6/60.6 kB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.5/4.5 MB\u001b[0m \u001b[31m71.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m54.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.5/60.5 MB\u001b[0m \u001b[31m8.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.9/84.9 kB\u001b[0m \u001b[31m9.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.3/3.3 MB\u001b[0m \u001b[31m69.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.9/43.9 MB\u001b[0m \u001b[31m17.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m26.5/26.5 MB\u001b[0m \u001b[31m47.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.3/2.3 MB\u001b[0m \u001b[31m85.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m904.1/904.1 kB\u001b[0m \u001b[31m61.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m228.9/228.9 kB\u001b[0m \u001b[31m23.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m214.3/214.3 kB\u001b[0m \u001b[31m22.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m149.2/149.2 kB\u001b[0m \u001b[31m21.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m131.9/131.9 kB\u001b[0m \u001b[31m17.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m88.2/88.2 kB\u001b[0m \u001b[31m13.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m498.1/498.1 MB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m156.1/156.1 MB\u001b[0m \u001b[31m7.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m51.1/51.1 kB\u001b[0m \u001b[31m7.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.7/50.7 kB\u001b[0m \u001b[31m6.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m832.9/832.9 kB\u001b[0m \u001b[31m73.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.6/42.6 kB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m79.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.8/5.8 MB\u001b[0m \u001b[31m104.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m462.3/462.3 kB\u001b[0m \u001b[31m51.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.4/1.4 MB\u001b[0m \u001b[31m92.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m197.1/197.1 kB\u001b[0m \u001b[31m27.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m112.2/112.2 kB\u001b[0m \u001b[31m16.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.9/4.9 MB\u001b[0m \u001b[31m123.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m781.3/781.3 kB\u001b[0m \u001b[31m68.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m485.6/485.6 kB\u001b[0m \u001b[31m48.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for pykalman (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for jsmin (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "flax 0.6.11 requires rich>=11.1, but you have rich 10.16.1 which is incompatible.\n",
            "tensorflow-datasets 4.9.2 requires protobuf>=3.20, but you have protobuf 3.19.6 which is incompatible.\n",
            "tensorflow-metadata 1.13.1 requires protobuf<5,>=3.20.3, but you have protobuf 3.19.6 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for nvidia-ml-py3 (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jftAOyvvuQeh",
        "outputId": "485713b6-7098-491d-fd4b-fc5478009dc4"
      },
      "source": [
        "import os\n",
        "import gc\n",
        "import random\n",
        "import time\n",
        "\n",
        "import nvidia_smi\n",
        "\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "\n",
        "import sleap\n",
        "\n",
        "sleap.versions()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "SLEAP: 1.3.1\n",
            "TensorFlow: 2.8.4\n",
            "Numpy: 1.22.4\n",
            "Python: 3.10.12\n",
            "OS: Linux-5.15.107+-x86_64-with-glibc2.31\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Utils"
      ],
      "metadata": {
        "id": "7TIJQcB2hz44"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def seed_everything(seed=42):\n",
        "    random.seed(seed)\n",
        "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
        "    np.random.seed(seed)\n",
        "    tf.random.set_seed(seed)\n",
        "    tf.keras.utils.set_random_seed(seed)\n",
        "\n",
        "def get_vram():\n",
        "    nvidia_smi.nvmlInit()\n",
        "\n",
        "    deviceCount = nvidia_smi.nvmlDeviceGetCount()\n",
        "    for i in range(deviceCount):\n",
        "        handle = nvidia_smi.nvmlDeviceGetHandleByIndex(i)\n",
        "        info = nvidia_smi.nvmlDeviceGetMemoryInfo(handle)\n",
        "        output = (\"Device {}: {}, Memory : ({:.2f}% free): {} (total), {} (free), {} (used)\"\n",
        "              .format(i, nvidia_smi.nvmlDeviceGetName(handle), 100*info.free/info.total,\n",
        "                      info.total/(1024 ** 3), info.free/(1024 ** 3), info.used/(1024 ** 3)))\n",
        "\n",
        "    nvidia_smi.nvmlShutdown()\n",
        "\n",
        "    return output\n",
        "\n",
        "def get_param_count(model):\n",
        "  trainable_params = np.sum([np.prod(v.get_shape()) for v in model.trainable_weights])\n",
        "  nontrainable_params = np.sum([np.prod(v.get_shape()) for v in model.non_trainable_weights])\n",
        "  total_params = trainable_params + nontrainable_params\n",
        "  return trainable_params, nontrainable_params, total_params"
      ],
      "metadata": {
        "id": "yxpqyaxnhzN9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "seed = 42\n",
        "seed_everything(seed)"
      ],
      "metadata": {
        "id": "h8FyuIUFh_if"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wSdTJYOdu4L6"
      },
      "source": [
        "# Download training data"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!curl -L --output labels.slp https://storage.googleapis.com/sleap-data/datasets/wt_gold.13pt/tracking_split2/train.pkg.slp\n",
        "!ls -lah\n",
        "\n",
        "!curl -L --output val_labels.slp https://storage.googleapis.com/sleap-data/datasets/wt_gold.13pt/tracking_split2/val.pkg.slp\n",
        "!ls -lah"
      ],
      "metadata": {
        "id": "UlQlnc4uM-U7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8e7b6743-f3e9-41e5-9a7e-cebe7c3edebb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "100  619M  100  619M    0     0  47.1M      0  0:00:13  0:00:13 --:--:-- 65.7M\n",
            "total 620M\n",
            "drwxr-xr-x 1 root root 4.0K Jun 26 17:21 .\n",
            "drwxr-xr-x 1 root root 4.0K Jun 26 17:18 ..\n",
            "drwxr-xr-x 4 root root 4.0K Jun 23 13:40 .config\n",
            "-rw-r--r-- 1 root root 620M Jun 26 17:21 labels.slp\n",
            "drwxr-xr-x 1 root root 4.0K Jun 23 13:41 sample_data\n",
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "100 77.2M  100 77.2M    0     0  22.8M      0  0:00:03  0:00:03 --:--:-- 22.8M\n",
            "total 697M\n",
            "drwxr-xr-x 1 root root 4.0K Jun 26 17:21 .\n",
            "drwxr-xr-x 1 root root 4.0K Jun 26 17:18 ..\n",
            "drwxr-xr-x 4 root root 4.0K Jun 23 13:40 .config\n",
            "-rw-r--r-- 1 root root 620M Jun 26 17:21 labels.slp\n",
            "drwxr-xr-x 1 root root 4.0K Jun 23 13:41 sample_data\n",
            "-rw-r--r-- 1 root root  78M Jun 26 17:21 val_labels.slp\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5lugn7a_HC0Q"
      },
      "source": [
        "# Load the training data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UMkSIZrTHCMr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "904ca43e-0be4-4999-dd6f-a59c6464a2c1"
      },
      "source": [
        "# SLEAP Labels files (.slp) can include the images as well as labeled instances and\n",
        "# other metadata for a project.\n",
        "labels = sleap.load_file(\"labels.slp\")\n",
        "labels = labels.with_user_labels_only()\n",
        "labels.describe()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Skeleton: Skeleton(description=None, nodes=[head, thorax, abdomen, wingL, wingR, forelegL4, forelegR4, midlegL4, midlegR4, hindlegL4, hindlegR4, eyeL, eyeR], edges=[thorax->head, thorax->abdomen, thorax->wingL, thorax->wingR, thorax->forelegL4, thorax->forelegR4, thorax->midlegL4, thorax->midlegR4, thorax->hindlegL4, thorax->hindlegR4, head->eyeL, head->eyeR], symmetries=[forelegL4<->forelegR4, hindlegL4<->hindlegR4, wingL<->wingR, midlegL4<->midlegR4, eyeL<->eyeR])\n",
            "Videos: ['labels.slp', 'labels.slp', 'labels.slp', 'labels.slp', 'labels.slp', 'labels.slp', 'labels.slp', 'labels.slp', 'labels.slp', 'labels.slp', 'labels.slp', 'labels.slp', 'labels.slp', 'labels.slp', 'labels.slp', 'labels.slp', 'labels.slp', 'labels.slp', 'labels.slp', 'labels.slp', 'labels.slp', 'labels.slp', 'labels.slp', 'labels.slp', 'labels.slp', 'labels.slp', 'labels.slp', 'labels.slp', 'labels.slp', 'labels.slp']\n",
            "Frames (user/predicted): 1,600/0\n",
            "Instances (user/predicted): 3,200/0\n",
            "Tracks: [Track(spawned_on=0, name='female'), Track(spawned_on=0, name='male')]\n",
            "Suggestions: 0\n",
            "Provenance: {}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Let's also do the same for the val labels.\n",
        "val_labels = sleap.load_file(\"val_labels.slp\")\n",
        "val_labels = val_labels.with_user_labels_only()\n",
        "val_labels.describe()"
      ],
      "metadata": {
        "id": "IanzIx9WsqoH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e1345d92-da0b-4983-f31c-74edb3dbef7b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Skeleton: Skeleton(description=None, nodes=[head, thorax, abdomen, wingL, wingR, forelegL4, forelegR4, midlegL4, midlegR4, hindlegL4, hindlegR4, eyeL, eyeR], edges=[thorax->head, thorax->abdomen, thorax->wingL, thorax->wingR, thorax->forelegL4, thorax->forelegR4, thorax->midlegL4, thorax->midlegR4, thorax->hindlegL4, thorax->hindlegR4, head->eyeL, head->eyeR], symmetries=[midlegL4<->midlegR4, forelegL4<->forelegR4, eyeL<->eyeR, hindlegL4<->hindlegR4, wingL<->wingR])\n",
            "Videos: ['val_labels.slp', 'val_labels.slp', 'val_labels.slp', 'val_labels.slp', 'val_labels.slp', 'val_labels.slp', 'val_labels.slp', 'val_labels.slp', 'val_labels.slp', 'val_labels.slp', 'val_labels.slp', 'val_labels.slp', 'val_labels.slp', 'val_labels.slp', 'val_labels.slp', 'val_labels.slp', 'val_labels.slp', 'val_labels.slp', 'val_labels.slp', 'val_labels.slp', 'val_labels.slp', 'val_labels.slp', 'val_labels.slp', 'val_labels.slp', 'val_labels.slp', 'val_labels.slp', 'val_labels.slp', 'val_labels.slp', 'val_labels.slp', 'val_labels.slp']\n",
            "Frames (user/predicted): 200/0\n",
            "Instances (user/predicted): 400/0\n",
            "Tracks: [Track(spawned_on=0, name='female'), Track(spawned_on=0, name='male')]\n",
            "Suggestions: 0\n",
            "Provenance: {}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EK8DDPHDHBr_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ffc06da2-ffcc-4fa4-b08e-5011a2ab31b0"
      },
      "source": [
        "# Labels are list-like containers whose elements are LabeledFrames\n",
        "print(f\"Number of labels: {len(labels)}\")\n",
        "\n",
        "labeled_frame = labels[0]\n",
        "labeled_frame"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of labels: 1600\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "LabeledFrame(video=HDF5Video('labels.slp'), frame_idx=166050, instances=2)"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pP5uXrU3HX6o",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "80dbe3db-9384-41df-f047-215b740f82c7"
      },
      "source": [
        "# LabeledFrames are containers for instances that were labeled in a single frame\n",
        "instance = labeled_frame[0]\n",
        "instance"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Instance(video=Video(filename=labels.slp, shape=(66, 1024, 1024, 1), backend=HDF5Video), frame_idx=166050, points=[head: (491.6, 187.7), thorax: (474.4, 224.8), abdomen: (459.9, 262.2), wingL: (448.3, 271.7), wingR: (452.1, 273.5), forelegL4: (478.5, 175.9), forelegR4: (499.9, 177.9), midlegL4: (440.6, 216.4), midlegR4: (510.1, 242.7), hindlegL4: (437.2, 234.3), hindlegR4: (490.9, 266.7), eyeL: (477.5, 193.2), eyeR: (498.4, 201.2)], track=Track(spawned_on=0, name='female'))"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "700fTPs4Hurg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1e7bdb39-1306-4b3e-c1b9-b360954d0679"
      },
      "source": [
        "# They can be converted to numpy arrays where each row corresponds to the coordinates\n",
        "# of a different body part:\n",
        "pts = instance.numpy()\n",
        "pts"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "rec.array([[491.58118169, 187.72078779],\n",
              "           [474.3603939 , 224.80196948],\n",
              "           [459.90098474, 262.16236338],\n",
              "           [448.26137864, 271.72078779],\n",
              "           [452.08118169, 273.54059084],\n",
              "           [478.5       , 175.90098474],\n",
              "           [499.94157558, 177.90098474],\n",
              "           [440.58118169, 216.3603939 ],\n",
              "           [510.12177253, 242.72078779],\n",
              "           [         nan,          nan],\n",
              "           [490.90098474, 266.72078779],\n",
              "           [477.54059084, 193.16236338],\n",
              "           [498.40098474, 201.18019695]],\n",
              "          dtype=float64)"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-vYsPusvviiu"
      },
      "source": [
        "# Setup training data generation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3SuKuK6GIDLg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a86e1652-7499-444b-dd52-4ff4da700e8f"
      },
      "source": [
        "# Initialize a pipeline from the labels.\n",
        "p = labels.with_user_labels_only().to_pipeline()\n",
        "\n",
        "# This pipeline will output dictionaries with tensors containing frame data:\n",
        "p.describe()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "         image: type=EagerTensor, shape=(1024, 1024, 1), dtype=tf.uint8, device=/job:localhost/replica:0/task:0/device:CPU:0\n",
            "raw_image_size: type=EagerTensor, shape=(3,), dtype=tf.int32, device=/job:localhost/replica:0/task:0/device:CPU:0\n",
            "   example_ind: type=EagerTensor, shape=(), dtype=tf.int64, device=/job:localhost/replica:0/task:0/device:CPU:0\n",
            "     video_ind: type=EagerTensor, shape=(), dtype=tf.int32, device=/job:localhost/replica:0/task:0/device:CPU:0\n",
            "     frame_ind: type=EagerTensor, shape=(), dtype=tf.int64, device=/job:localhost/replica:0/task:0/device:CPU:0\n",
            "         scale: type=EagerTensor, shape=(2,), dtype=tf.float32, device=/job:localhost/replica:0/task:0/device:CPU:0\n",
            "     instances: type=EagerTensor, shape=(2, 13, 2), dtype=tf.float32, device=/job:localhost/replica:0/task:0/device:CPU:0\n",
            " skeleton_inds: type=EagerTensor, shape=(2,), dtype=tf.int32, device=/job:localhost/replica:0/task:0/device:CPU:0\n",
            "    track_inds: type=EagerTensor, shape=(2,), dtype=tf.int32, device=/job:localhost/replica:0/task:0/device:CPU:0\n",
            "      n_tracks: type=EagerTensor, shape=(), dtype=tf.int32, device=/job:localhost/replica:0/task:0/device:CPU:0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ozFr7JQeI22M",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6d662506-a351-4e2f-dc06-535ef9add82f"
      },
      "source": [
        "# Let's add some transformations necessary for the centered-instance model.\n",
        "p = labels.with_user_labels_only().to_pipeline()\n",
        "p += sleap.pipelines.ImgaugAugmenter.from_config(sleap.pipelines.AugmentationConfig(rotate=True, rotation_min_angle=-180, rotation_max_angle=180))\n",
        "p += sleap.pipelines.Normalizer()\n",
        "p += sleap.pipelines.InstanceCentroidFinder(center_on_anchor_part=True, anchor_part_names=\"thorax\", skeletons=labels.skeletons)\n",
        "p += sleap.pipelines.InstanceCropper(crop_width=160, crop_height=160)\n",
        "p += sleap.pipelines.InstanceConfidenceMapGenerator(sigma=1.5, output_stride=2)\n",
        "p += sleap.pipelines.Batcher(batch_size=4, drop_remainder=True)\n",
        "p.describe()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "          instance_image: type=EagerTensor, shape=(4, 160, 160, 1), dtype=tf.float32, device=/job:localhost/replica:0/task:0/device:CPU:0\n",
            "                    bbox: type=EagerTensor, shape=(4, 4), dtype=tf.float32, device=/job:localhost/replica:0/task:0/device:CPU:0\n",
            "         center_instance: type=EagerTensor, shape=(4, 13, 2), dtype=tf.float32, device=/job:localhost/replica:0/task:0/device:CPU:0\n",
            "     center_instance_ind: type=EagerTensor, shape=(4, 1), dtype=tf.int32, device=/job:localhost/replica:0/task:0/device:CPU:0\n",
            "               track_ind: type=EagerTensor, shape=(4, 1), dtype=tf.int32, device=/job:localhost/replica:0/task:0/device:CPU:0\n",
            "           all_instances: type=EagerTensor, shape=(4, 2, 13, 2), dtype=tf.float32, device=/job:localhost/replica:0/task:0/device:CPU:0\n",
            "                centroid: type=EagerTensor, shape=(4, 2), dtype=tf.float32, device=/job:localhost/replica:0/task:0/device:CPU:0\n",
            "       full_image_height: type=EagerTensor, shape=(4, 1), dtype=tf.int32, device=/job:localhost/replica:0/task:0/device:CPU:0\n",
            "        full_image_width: type=EagerTensor, shape=(4, 1), dtype=tf.int32, device=/job:localhost/replica:0/task:0/device:CPU:0\n",
            "          raw_image_size: type=EagerTensor, shape=(4, 3), dtype=tf.int32, device=/job:localhost/replica:0/task:0/device:CPU:0\n",
            "             example_ind: type=EagerTensor, shape=(4, 1), dtype=tf.int64, device=/job:localhost/replica:0/task:0/device:CPU:0\n",
            "               video_ind: type=EagerTensor, shape=(4, 1), dtype=tf.int32, device=/job:localhost/replica:0/task:0/device:CPU:0\n",
            "               frame_ind: type=EagerTensor, shape=(4, 1), dtype=tf.int64, device=/job:localhost/replica:0/task:0/device:CPU:0\n",
            "                   scale: type=EagerTensor, shape=(4, 2), dtype=tf.float32, device=/job:localhost/replica:0/task:0/device:CPU:0\n",
            "           skeleton_inds: type=EagerTensor, shape=(4, 2), dtype=tf.int32, device=/job:localhost/replica:0/task:0/device:CPU:0\n",
            "              track_inds: type=EagerTensor, shape=(4, 2), dtype=tf.int32, device=/job:localhost/replica:0/task:0/device:CPU:0\n",
            "                n_tracks: type=EagerTensor, shape=(4, 1), dtype=tf.int32, device=/job:localhost/replica:0/task:0/device:CPU:0\n",
            "instance_confidence_maps: type=EagerTensor, shape=(4, 80, 80, 13), dtype=tf.float32, device=/job:localhost/replica:0/task:0/device:CPU:0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Let's build our validation pipeline.\n",
        "# Note, we didn't include the augmentations.\n",
        "val_p = labels.with_user_labels_only().to_pipeline()\n",
        "val_p += sleap.pipelines.Normalizer()\n",
        "val_p += sleap.pipelines.InstanceCentroidFinder(center_on_anchor_part=True, anchor_part_names=\"thorax\", skeletons=labels.skeletons)\n",
        "val_p += sleap.pipelines.InstanceCropper(crop_width=160, crop_height=160)\n",
        "val_p += sleap.pipelines.InstanceConfidenceMapGenerator(sigma=1.5, output_stride=2)\n",
        "val_p += sleap.pipelines.Batcher(batch_size=4, drop_remainder=True)\n",
        "val_p.describe()"
      ],
      "metadata": {
        "id": "lJRdies-tWuK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a4888811-f5f1-4b4f-c593-14b2dd089c01"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "          instance_image: type=EagerTensor, shape=(4, 160, 160, 1), dtype=tf.float32, device=/job:localhost/replica:0/task:0/device:CPU:0\n",
            "                    bbox: type=EagerTensor, shape=(4, 4), dtype=tf.float32, device=/job:localhost/replica:0/task:0/device:CPU:0\n",
            "         center_instance: type=EagerTensor, shape=(4, 13, 2), dtype=tf.float32, device=/job:localhost/replica:0/task:0/device:CPU:0\n",
            "     center_instance_ind: type=EagerTensor, shape=(4, 1), dtype=tf.int32, device=/job:localhost/replica:0/task:0/device:CPU:0\n",
            "               track_ind: type=EagerTensor, shape=(4, 1), dtype=tf.int32, device=/job:localhost/replica:0/task:0/device:CPU:0\n",
            "           all_instances: type=EagerTensor, shape=(4, 2, 13, 2), dtype=tf.float32, device=/job:localhost/replica:0/task:0/device:CPU:0\n",
            "                centroid: type=EagerTensor, shape=(4, 2), dtype=tf.float32, device=/job:localhost/replica:0/task:0/device:CPU:0\n",
            "       full_image_height: type=EagerTensor, shape=(4, 1), dtype=tf.int32, device=/job:localhost/replica:0/task:0/device:CPU:0\n",
            "        full_image_width: type=EagerTensor, shape=(4, 1), dtype=tf.int32, device=/job:localhost/replica:0/task:0/device:CPU:0\n",
            "          raw_image_size: type=EagerTensor, shape=(4, 3), dtype=tf.int32, device=/job:localhost/replica:0/task:0/device:CPU:0\n",
            "             example_ind: type=EagerTensor, shape=(4, 1), dtype=tf.int64, device=/job:localhost/replica:0/task:0/device:CPU:0\n",
            "               video_ind: type=EagerTensor, shape=(4, 1), dtype=tf.int32, device=/job:localhost/replica:0/task:0/device:CPU:0\n",
            "               frame_ind: type=EagerTensor, shape=(4, 1), dtype=tf.int64, device=/job:localhost/replica:0/task:0/device:CPU:0\n",
            "                   scale: type=EagerTensor, shape=(4, 2), dtype=tf.float32, device=/job:localhost/replica:0/task:0/device:CPU:0\n",
            "           skeleton_inds: type=EagerTensor, shape=(4, 2), dtype=tf.int32, device=/job:localhost/replica:0/task:0/device:CPU:0\n",
            "              track_inds: type=EagerTensor, shape=(4, 2), dtype=tf.int32, device=/job:localhost/replica:0/task:0/device:CPU:0\n",
            "                n_tracks: type=EagerTensor, shape=(4, 1), dtype=tf.int32, device=/job:localhost/replica:0/task:0/device:CPU:0\n",
            "instance_confidence_maps: type=EagerTensor, shape=(4, 80, 80, 13), dtype=tf.float32, device=/job:localhost/replica:0/task:0/device:CPU:0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0yo95XzRKkba"
      },
      "source": [
        "# Setting up a neural network model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kjshAqdqKyUI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2f775f15-9480-438a-c5a4-b99d481225a6"
      },
      "source": [
        "# Instantiate the backbone builder.\n",
        "unet = sleap.nn.architectures.unet.UNet(filters=32, filters_rate=1.5, down_blocks=4, up_blocks=3, up_interpolate=True)\n",
        "\n",
        "# Create the input layer (see above for the dimensions)\n",
        "x_in = tf.keras.layers.Input((160, 160, 1))\n",
        "\n",
        "# Create the feature extractor backbone.\n",
        "x_features, x_intermediate = unet.make_backbone(x_in)\n",
        "\n",
        "# Do a 1x1 conv with linear activation to remap activations to the number of channels in\n",
        "# the confidence maps (see above)\n",
        "x_confmaps = tf.keras.layers.Conv2D(filters=13, kernel_size=1, strides=1, padding=\"same\")(x_features)\n",
        "\n",
        "# Create a Model that links the whole graph\n",
        "model = tf.keras.Model(x_in, x_confmaps)\n",
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_1 (InputLayer)           [(None, 160, 160, 1  0           []                               \n",
            "                                )]                                                                \n",
            "                                                                                                  \n",
            " stack0_enc0_conv0 (Conv2D)     (None, 160, 160, 32  320         ['input_1[0][0]']                \n",
            "                                )                                                                 \n",
            "                                                                                                  \n",
            " stack0_enc0_act0_relu (Activat  (None, 160, 160, 32  0          ['stack0_enc0_conv0[0][0]']      \n",
            " ion)                           )                                                                 \n",
            "                                                                                                  \n",
            " stack0_enc0_conv1 (Conv2D)     (None, 160, 160, 32  9248        ['stack0_enc0_act0_relu[0][0]']  \n",
            "                                )                                                                 \n",
            "                                                                                                  \n",
            " stack0_enc0_act1_relu (Activat  (None, 160, 160, 32  0          ['stack0_enc0_conv1[0][0]']      \n",
            " ion)                           )                                                                 \n",
            "                                                                                                  \n",
            " stack0_enc1_pool (MaxPooling2D  (None, 80, 80, 32)  0           ['stack0_enc0_act1_relu[0][0]']  \n",
            " )                                                                                                \n",
            "                                                                                                  \n",
            " stack0_enc1_conv0 (Conv2D)     (None, 80, 80, 48)   13872       ['stack0_enc1_pool[0][0]']       \n",
            "                                                                                                  \n",
            " stack0_enc1_act0_relu (Activat  (None, 80, 80, 48)  0           ['stack0_enc1_conv0[0][0]']      \n",
            " ion)                                                                                             \n",
            "                                                                                                  \n",
            " stack0_enc1_conv1 (Conv2D)     (None, 80, 80, 48)   20784       ['stack0_enc1_act0_relu[0][0]']  \n",
            "                                                                                                  \n",
            " stack0_enc1_act1_relu (Activat  (None, 80, 80, 48)  0           ['stack0_enc1_conv1[0][0]']      \n",
            " ion)                                                                                             \n",
            "                                                                                                  \n",
            " stack0_enc2_pool (MaxPooling2D  (None, 40, 40, 48)  0           ['stack0_enc1_act1_relu[0][0]']  \n",
            " )                                                                                                \n",
            "                                                                                                  \n",
            " stack0_enc2_conv0 (Conv2D)     (None, 40, 40, 72)   31176       ['stack0_enc2_pool[0][0]']       \n",
            "                                                                                                  \n",
            " stack0_enc2_act0_relu (Activat  (None, 40, 40, 72)  0           ['stack0_enc2_conv0[0][0]']      \n",
            " ion)                                                                                             \n",
            "                                                                                                  \n",
            " stack0_enc2_conv1 (Conv2D)     (None, 40, 40, 72)   46728       ['stack0_enc2_act0_relu[0][0]']  \n",
            "                                                                                                  \n",
            " stack0_enc2_act1_relu (Activat  (None, 40, 40, 72)  0           ['stack0_enc2_conv1[0][0]']      \n",
            " ion)                                                                                             \n",
            "                                                                                                  \n",
            " stack0_enc3_pool (MaxPooling2D  (None, 20, 20, 72)  0           ['stack0_enc2_act1_relu[0][0]']  \n",
            " )                                                                                                \n",
            "                                                                                                  \n",
            " stack0_enc3_conv0 (Conv2D)     (None, 20, 20, 108)  70092       ['stack0_enc3_pool[0][0]']       \n",
            "                                                                                                  \n",
            " stack0_enc3_act0_relu (Activat  (None, 20, 20, 108)  0          ['stack0_enc3_conv0[0][0]']      \n",
            " ion)                                                                                             \n",
            "                                                                                                  \n",
            " stack0_enc3_conv1 (Conv2D)     (None, 20, 20, 108)  105084      ['stack0_enc3_act0_relu[0][0]']  \n",
            "                                                                                                  \n",
            " stack0_enc3_act1_relu (Activat  (None, 20, 20, 108)  0          ['stack0_enc3_conv1[0][0]']      \n",
            " ion)                                                                                             \n",
            "                                                                                                  \n",
            " stack0_enc4_last_pool (MaxPool  (None, 10, 10, 108)  0          ['stack0_enc3_act1_relu[0][0]']  \n",
            " ing2D)                                                                                           \n",
            "                                                                                                  \n",
            " stack0_enc5_middle_expand_conv  (None, 10, 10, 162)  157626     ['stack0_enc4_last_pool[0][0]']  \n",
            " 0 (Conv2D)                                                                                       \n",
            "                                                                                                  \n",
            " stack0_enc5_middle_expand_act0  (None, 10, 10, 162)  0          ['stack0_enc5_middle_expand_conv0\n",
            " _relu (Activation)                                              [0][0]']                         \n",
            "                                                                                                  \n",
            " stack0_enc6_middle_contract_co  (None, 10, 10, 162)  236358     ['stack0_enc5_middle_expand_act0_\n",
            " nv0 (Conv2D)                                                    relu[0][0]']                     \n",
            "                                                                                                  \n",
            " stack0_enc6_middle_contract_ac  (None, 10, 10, 162)  0          ['stack0_enc6_middle_contract_con\n",
            " t0_relu (Activation)                                            v0[0][0]']                       \n",
            "                                                                                                  \n",
            " stack0_dec0_s16_to_s8_interp_b  (None, 20, 20, 162)  0          ['stack0_enc6_middle_contract_act\n",
            " ilinear (UpSampling2D)                                          0_relu[0][0]']                   \n",
            "                                                                                                  \n",
            " stack0_dec0_s16_to_s8_skip_con  (None, 20, 20, 270)  0          ['stack0_enc3_act1_relu[0][0]',  \n",
            " cat (Concatenate)                                                'stack0_dec0_s16_to_s8_interp_bi\n",
            "                                                                 linear[0][0]']                   \n",
            "                                                                                                  \n",
            " stack0_dec0_s16_to_s8_refine_c  (None, 20, 20, 108)  262548     ['stack0_dec0_s16_to_s8_skip_conc\n",
            " onv0 (Conv2D)                                                   at[0][0]']                       \n",
            "                                                                                                  \n",
            " stack0_dec0_s16_to_s8_refine_c  (None, 20, 20, 108)  0          ['stack0_dec0_s16_to_s8_refine_co\n",
            " onv0_act_relu (Activation)                                      nv0[0][0]']                      \n",
            "                                                                                                  \n",
            " stack0_dec0_s16_to_s8_refine_c  (None, 20, 20, 108)  105084     ['stack0_dec0_s16_to_s8_refine_co\n",
            " onv1 (Conv2D)                                                   nv0_act_relu[0][0]']             \n",
            "                                                                                                  \n",
            " stack0_dec0_s16_to_s8_refine_c  (None, 20, 20, 108)  0          ['stack0_dec0_s16_to_s8_refine_co\n",
            " onv1_act_relu (Activation)                                      nv1[0][0]']                      \n",
            "                                                                                                  \n",
            " stack0_dec1_s8_to_s4_interp_bi  (None, 40, 40, 108)  0          ['stack0_dec0_s16_to_s8_refine_co\n",
            " linear (UpSampling2D)                                           nv1_act_relu[0][0]']             \n",
            "                                                                                                  \n",
            " stack0_dec1_s8_to_s4_skip_conc  (None, 40, 40, 180)  0          ['stack0_enc2_act1_relu[0][0]',  \n",
            " at (Concatenate)                                                 'stack0_dec1_s8_to_s4_interp_bil\n",
            "                                                                 inear[0][0]']                    \n",
            "                                                                                                  \n",
            " stack0_dec1_s8_to_s4_refine_co  (None, 40, 40, 72)  116712      ['stack0_dec1_s8_to_s4_skip_conca\n",
            " nv0 (Conv2D)                                                    t[0][0]']                        \n",
            "                                                                                                  \n",
            " stack0_dec1_s8_to_s4_refine_co  (None, 40, 40, 72)  0           ['stack0_dec1_s8_to_s4_refine_con\n",
            " nv0_act_relu (Activation)                                       v0[0][0]']                       \n",
            "                                                                                                  \n",
            " stack0_dec1_s8_to_s4_refine_co  (None, 40, 40, 72)  46728       ['stack0_dec1_s8_to_s4_refine_con\n",
            " nv1 (Conv2D)                                                    v0_act_relu[0][0]']              \n",
            "                                                                                                  \n",
            " stack0_dec1_s8_to_s4_refine_co  (None, 40, 40, 72)  0           ['stack0_dec1_s8_to_s4_refine_con\n",
            " nv1_act_relu (Activation)                                       v1[0][0]']                       \n",
            "                                                                                                  \n",
            " stack0_dec2_s4_to_s2_interp_bi  (None, 80, 80, 72)  0           ['stack0_dec1_s8_to_s4_refine_con\n",
            " linear (UpSampling2D)                                           v1_act_relu[0][0]']              \n",
            "                                                                                                  \n",
            " stack0_dec2_s4_to_s2_skip_conc  (None, 80, 80, 120)  0          ['stack0_enc1_act1_relu[0][0]',  \n",
            " at (Concatenate)                                                 'stack0_dec2_s4_to_s2_interp_bil\n",
            "                                                                 inear[0][0]']                    \n",
            "                                                                                                  \n",
            " stack0_dec2_s4_to_s2_refine_co  (None, 80, 80, 48)  51888       ['stack0_dec2_s4_to_s2_skip_conca\n",
            " nv0 (Conv2D)                                                    t[0][0]']                        \n",
            "                                                                                                  \n",
            " stack0_dec2_s4_to_s2_refine_co  (None, 80, 80, 48)  0           ['stack0_dec2_s4_to_s2_refine_con\n",
            " nv0_act_relu (Activation)                                       v0[0][0]']                       \n",
            "                                                                                                  \n",
            " stack0_dec2_s4_to_s2_refine_co  (None, 80, 80, 48)  20784       ['stack0_dec2_s4_to_s2_refine_con\n",
            " nv1 (Conv2D)                                                    v0_act_relu[0][0]']              \n",
            "                                                                                                  \n",
            " stack0_dec2_s4_to_s2_refine_co  (None, 80, 80, 48)  0           ['stack0_dec2_s4_to_s2_refine_con\n",
            " nv1_act_relu (Activation)                                       v1[0][0]']                       \n",
            "                                                                                                  \n",
            " conv2d (Conv2D)                (None, 80, 80, 13)   637         ['stack0_dec2_s4_to_s2_refine_con\n",
            "                                                                 v1_act_relu[0][0]']              \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 1,295,669\n",
            "Trainable params: 1,295,669\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gEfgZT4hLyW4"
      },
      "source": [
        "# Train the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3IKZi6FRL5e_"
      },
      "source": [
        "# Setup the optimizer and loss function.\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate=1e-4)\n",
        "loss_fn = tf.keras.losses.MeanSquaredError()\n",
        "\n",
        "# Define a \"training step\" function. This does the forward/backward passes and applies\n",
        "# the gradients to update the model weights.\n",
        "@tf.function\n",
        "def train_step(ex, model, optimizer, loss_fn):\n",
        "    with tf.GradientTape() as tape:\n",
        "        predicted_confmaps = model(ex[\"instance_image\"])\n",
        "        loss = loss_fn(ex[\"instance_confidence_maps\"], predicted_confmaps)\n",
        "\n",
        "    grads = tape.gradient(loss, model.trainable_weights)\n",
        "    optimizer.apply_gradients(zip(grads, model.trainable_weights))\n",
        "\n",
        "    return loss\n",
        "\n",
        "@tf.function\n",
        "def val_step(ex, model, loss_fn):\n",
        "    predicted_confmaps = model(ex[\"instance_image\"])\n",
        "    loss = loss_fn(ex[\"instance_confidence_maps\"], predicted_confmaps)\n",
        "\n",
        "    return loss"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "unet = sleap.nn.architectures.unet.UNet(filters=32, filters_rate=1.5, down_blocks=4, up_blocks=3, up_interpolate=True)\n",
        "x_in = tf.keras.layers.Input((160, 160, 1))\n",
        "x_features, x_intermediate = unet.make_backbone(x_in)\n",
        "x_confmaps = tf.keras.layers.Conv2D(filters=13, kernel_size=1, strides=1, padding=\"same\")(x_features)\n",
        "model = tf.keras.Model(x_in, x_confmaps)\n",
        "\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate=1e-4)\n",
        "loss_fn = tf.keras.losses.MeanSquaredError()\n",
        "\n",
        "# Training loop, go!\n",
        "epochs = 3\n",
        "for epoch in range(epochs):\n",
        "    start_time = time.time()\n",
        "    train_loss = 0\n",
        "    for step, ex in enumerate(p.make_dataset()):\n",
        "        loss = train_step(ex, model, optimizer, loss_fn)\n",
        "\n",
        "        if step % 100 == 0:\n",
        "            print(f\"Epoch {epoch:03d} | Step {step:03d} | loss = {loss:.5f}\")\n",
        "\n",
        "        train_loss += loss\n",
        "\n",
        "    train_loss /= (step+1)\n",
        "    train_time = time.time() - start_time\n",
        "    print(f\"TRAIN: --- {train_time}s seconds ---\")\n",
        "\n",
        "    start_time = time.time()\n",
        "    val_loss = 0\n",
        "    for step, ex in enumerate(val_p.make_dataset()):\n",
        "        loss = val_step(ex, model, loss_fn)\n",
        "        val_loss += loss\n",
        "\n",
        "    val_loss /= (step+1)\n",
        "    val_time = time.time() - start_time\n",
        "    print(f\"VAL: --- {val_time}s seconds ---\")"
      ],
      "metadata": {
        "id": "goZvpYV8Xe0l",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "77ae2e60-3500-4b26-8397-50c732a42b07"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 000 | Step 000 | loss = 0.00120\n",
            "Epoch 000 | Step 100 | loss = 0.00094\n",
            "Epoch 000 | Step 200 | loss = 0.00104\n",
            "Epoch 000 | Step 300 | loss = 0.00103\n",
            "Epoch 000 | Step 400 | loss = 0.00100\n",
            "Epoch 000 | Step 500 | loss = 0.00098\n",
            "Epoch 000 | Step 600 | loss = 0.00097\n",
            "Epoch 000 | Step 700 | loss = 0.00092\n",
            "TRAIN: --- 69.75044798851013s seconds ---\n",
            "VAL: --- 42.28296756744385s seconds ---\n",
            "Epoch 001 | Step 000 | loss = 0.00084\n",
            "Epoch 001 | Step 100 | loss = 0.00076\n",
            "Epoch 001 | Step 200 | loss = 0.00070\n",
            "Epoch 001 | Step 300 | loss = 0.00066\n",
            "Epoch 001 | Step 400 | loss = 0.00068\n",
            "Epoch 001 | Step 500 | loss = 0.00066\n",
            "Epoch 001 | Step 600 | loss = 0.00082\n",
            "Epoch 001 | Step 700 | loss = 0.00064\n",
            "TRAIN: --- 61.75702166557312s seconds ---\n",
            "VAL: --- 37.68329906463623s seconds ---\n",
            "Epoch 002 | Step 000 | loss = 0.00060\n",
            "Epoch 002 | Step 100 | loss = 0.00065\n",
            "Epoch 002 | Step 200 | loss = 0.00047\n",
            "Epoch 002 | Step 300 | loss = 0.00047\n",
            "Epoch 002 | Step 400 | loss = 0.00059\n",
            "Epoch 002 | Step 500 | loss = 0.00052\n",
            "Epoch 002 | Step 600 | loss = 0.00080\n",
            "Epoch 002 | Step 700 | loss = 0.00052\n",
            "TRAIN: --- 83.33306431770325s seconds ---\n",
            "VAL: --- 42.728052854537964s seconds ---\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Inference & Evaluation"
      ],
      "metadata": {
        "id": "FT4cFegzmv2v"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Utility Functions"
      ],
      "metadata": {
        "id": "axZAZu3yv9uY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def make_grid_vectors(\n",
        "    image_height: int, image_width: int, output_stride: int = 1):\n",
        "\n",
        "    xv = np.arange(0, image_width, step=output_stride)\n",
        "    yv = np.arange(0, image_height, step=output_stride)\n",
        "    return xv, yv"
      ],
      "metadata": {
        "id": "mfA1kuQsv9Of"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. find instance peaks\n",
        "def find_peaks(cms, xv, yv):\n",
        "    \"\"\"Find peaks in a set of confidence maps via integral regression.\n",
        "\n",
        "    Args:\n",
        "        cms: A batch of confidence maps of shape (batch_size, height, width, n_points).\n",
        "        xv: X-sampling vector of shape (grid_width,).\n",
        "        yv: Y-sampling vector of shape (grid_width,).\n",
        "\n",
        "    Returns:\n",
        "        A set of estimated peaks of shape (batch_size, n_points, 2).\n",
        "\n",
        "    Notes:\n",
        "        This function can also accept confidence maps of shape (height, width, n_points)\n",
        "        and returns peaks as (n_points, 2).\n",
        "    \"\"\"\n",
        "    is_singleton = cms.ndim == 3\n",
        "    if is_singleton:\n",
        "        cms = np.expand_dims(cms, axis=0)\n",
        "\n",
        "    # Find integral over height and width.\n",
        "    z = cms.reshape(cms.shape[0], -1, cms.shape[-1]).sum(axis=1)\n",
        "\n",
        "    # Compute x- and y-coordinates.\n",
        "    x = (cms * xv.reshape(1, 1, -1, 1)).reshape(cms.shape[0], -1, cms.shape[-1]).sum(axis=1) / z\n",
        "    y = (cms * yv.reshape(1, -1, 1, 1)).reshape(cms.shape[0], -1, cms.shape[-1]).sum(axis=1) / z\n",
        "\n",
        "    # Stack the coordinates into (batch_size, n_points, 2).\n",
        "    pts_pr = np.stack([x, y], axis=-1)\n",
        "\n",
        "    if is_singleton:\n",
        "        pts_pr = pts_pr.squeeze(axis=0)\n",
        "    return pts_pr\n",
        "\n",
        "def clip_peaks(peaks):\n",
        "  for p in peaks:\n",
        "    out_of_bounds = np.logical_not(np.logical_and(np.all(np.where(0 <= p, True, False), axis=1),\n",
        "                                                  np.all(np.where(p <= 160, True, False), axis=1)))\n",
        "    p[out_of_bounds, :] = np.nan\n",
        "\n",
        "  return peaks"
      ],
      "metadata": {
        "id": "Hf954-WvrDkT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_instance_area(points: np.ndarray) -> np.ndarray:\n",
        "    \"\"\"Compute the area of the bounding box of a set of keypoints.\n",
        "\n",
        "    Args:\n",
        "        points: A numpy array of coordinates.\n",
        "\n",
        "    Returns:\n",
        "        The area of the bounding box of the points.\n",
        "    \"\"\"\n",
        "    if points.ndim == 2:\n",
        "        points = np.expand_dims(points, axis=0)\n",
        "\n",
        "    min_pt = np.nanmin(points, axis=-2)\n",
        "    max_pt = np.nanmax(points, axis=-2)\n",
        "\n",
        "    return np.prod(max_pt - min_pt, axis=-1)\n",
        "\n",
        "\n",
        "\n",
        "def compute_oks(\n",
        "    points_gt: np.ndarray,\n",
        "    points_pr: np.ndarray,\n",
        "    scale = None,\n",
        "    stddev: float = 0.025,\n",
        ") -> np.ndarray:\n",
        "    \"\"\"Compute the object keypoints similarity between sets of points.\n",
        "\n",
        "    Args:\n",
        "        points_gt: Ground truth instances of shape (n_gt, n_nodes, n_ed),\n",
        "            where n_nodes is the number of body parts/keypoint types, and n_ed\n",
        "            is the number of Euclidean dimensions (typically 2 or 3). Keypoints\n",
        "            that are missing/not visible should be represented as NaNs.\n",
        "        points_pr: Predicted instance of shape (n_pr, n_nodes, n_ed).\n",
        "        scale: Size scaling factor to use when weighing the scores, typically\n",
        "            the area of the bounding box of the instance (in pixels). This\n",
        "            should be of the length n_gt. If a scalar is provided, the same\n",
        "            number is used for all ground truth instances. If set to None, the\n",
        "            bounding box area of the ground truth instances will be calculated.\n",
        "        stddev: The standard deviation associated with the spread in the\n",
        "            localization accuracy of each node/keypoint type. This should be of\n",
        "            the length n_nodes. \"Easier\" keypoint types will have lower values\n",
        "            to reflect the smaller spread expected in localizing it.\n",
        "\n",
        "    Returns:\n",
        "        The object keypoints similarity between every pair of ground truth and\n",
        "        predicted instance, a numpy array of of shape (n_gt, n_pr) in the range\n",
        "        of [0, 1.0], with 1.0 denoting a perfect match.\n",
        "\n",
        "    Notes:\n",
        "        It's important to set the stddev appropriately when accounting for the\n",
        "        difficulty of each keypoint type. For reference, the median value for\n",
        "        all keypoint types in COCO is 0.072. The \"easiest\" keypoint is the left\n",
        "        eye, with stddev of 0.025, since it is easy to precisely locate the\n",
        "        eyes when labeling. The \"hardest\" keypoint is the left hip, with stddev\n",
        "        of 0.107, since it's hard to locate the left hip bone without external\n",
        "        anatomical features and since it is often occluded by clothing.\n",
        "\n",
        "        The implementation here is based off of the descriptions in:\n",
        "        Ronch & Perona. \"Benchmarking and Error Diagnosis in Multi-Instance Pose\n",
        "        Estimation.\" ICCV (2017).\n",
        "    \"\"\"\n",
        "    if points_gt.ndim == 2:\n",
        "        points_gt = np.expand_dims(points_gt, axis=0)\n",
        "    if points_pr.ndim == 2:\n",
        "        points_pr = np.expand_dims(points_pr, axis=0)\n",
        "\n",
        "    if scale is None:\n",
        "        scale = compute_instance_area(points_gt)\n",
        "\n",
        "    n_gt, n_nodes, n_ed = points_gt.shape  # n_ed = 2 or 3 (euclidean dimensions)\n",
        "    n_pr = points_pr.shape[0]\n",
        "\n",
        "    # If scalar scale was provided, use the same for each ground truth instance.\n",
        "    if np.isscalar(scale):\n",
        "        scale = np.full(n_gt, scale)\n",
        "\n",
        "    # If scalar standard deviation was provided, use the same for each node.\n",
        "    if np.isscalar(stddev):\n",
        "        stddev = np.full(n_nodes, stddev)\n",
        "\n",
        "    # Compute displacement between each pair.\n",
        "    displacement = np.reshape(points_gt, (n_gt, 1, n_nodes, n_ed)) - np.reshape(\n",
        "        points_pr, (1, n_pr, n_nodes, n_ed)\n",
        "    )\n",
        "    assert displacement.shape == (n_gt, n_pr, n_nodes, n_ed)\n",
        "\n",
        "    # Convert to pairwise Euclidean distances.\n",
        "    distance = (displacement ** 2).sum(axis=-1)  # (n_gt, n_pr, n_nodes)\n",
        "    assert distance.shape == (n_gt, n_pr, n_nodes)\n",
        "\n",
        "    # Compute the normalization factor per keypoint.\n",
        "    spread_factor = (2 * stddev) ** 2\n",
        "    scale_factor = 2 * (scale + np.spacing(1))\n",
        "    normalization_factor = np.reshape(spread_factor, (1, 1, n_nodes)) * np.reshape(\n",
        "        scale_factor, (n_gt, 1, 1)\n",
        "    )\n",
        "    assert normalization_factor.shape == (n_gt, 1, n_nodes)\n",
        "\n",
        "    # Since a \"miss\" is considered as KS < 0.5, we'll set the\n",
        "    # distances for predicted points that are missing to inf.\n",
        "    missing_pr = np.any(np.isnan(points_pr), axis=-1)  # (n_pr, n_nodes)\n",
        "    assert missing_pr.shape == (n_pr, n_nodes)\n",
        "    distance[:, missing_pr] = np.inf\n",
        "\n",
        "    # Compute the keypoint similarity as per the top of Eq. 1.\n",
        "    ks = np.exp(-(distance / normalization_factor))  # (n_gt, n_pr, n_nodes)\n",
        "    assert ks.shape == (n_gt, n_pr, n_nodes)\n",
        "\n",
        "    # Set the KS for missing ground truth points to 0.\n",
        "    # This is equivalent to the visibility delta function of the bottom\n",
        "    # of Eq. 1.\n",
        "    missing_gt = np.any(np.isnan(points_gt), axis=-1)  # (n_gt, n_nodes)\n",
        "    assert missing_gt.shape == (n_gt, n_nodes)\n",
        "    ks[np.expand_dims(missing_gt, axis=1)] = 0\n",
        "\n",
        "    # Compute the OKS.\n",
        "    n_visible_gt = np.sum(\n",
        "        (~missing_gt).astype(\"float64\"), axis=-1, keepdims=True\n",
        "    )  # (n_gt, 1)\n",
        "    oks = np.sum(ks, axis=-1) / n_visible_gt\n",
        "    assert oks.shape == (n_gt, n_pr)\n",
        "\n",
        "    return oks"
      ],
      "metadata": {
        "id": "6y17X7_c1xKi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_match_scores(all_y_preds, all_y_gt, stddev=0.25):\n",
        "  match_scores = []\n",
        "  for i, j in zip(all_y_gt, all_y_preds):\n",
        "    oks = compute_oks(i, j, stddev=stddev)[0][0]\n",
        "    match_scores.append(oks)\n",
        "\n",
        "  return match_scores\n",
        "\n",
        "def evaluate(\n",
        "  match_scores,\n",
        "  num_positive_pairs,\n",
        "  num_false_negatives,\n",
        "  recall_thresholds,\n",
        "  match_score_thresholds,\n",
        "):\n",
        "\n",
        "  precisions = []\n",
        "  recalls = []\n",
        "\n",
        "  npig = num_positive_pairs + num_false_negatives  # total number of GT instances\n",
        "\n",
        "  for match_score_threshold in match_score_thresholds:\n",
        "\n",
        "      tp = np.cumsum(match_scores >= match_score_threshold)\n",
        "      fp = np.cumsum(match_scores < match_score_threshold)\n",
        "\n",
        "      rc = tp / npig\n",
        "      pr = tp / (fp + tp + np.spacing(1))\n",
        "\n",
        "      recall = rc[-1]  # best recall at this OKS threshold\n",
        "\n",
        "      # Ensure strictly decreasing precisions.\n",
        "      for i in range(len(pr) - 1, 0, -1):\n",
        "          if pr[i] > pr[i - 1]:\n",
        "              pr[i - 1] = pr[i]\n",
        "\n",
        "      # Find best precision at each recall threshold.\n",
        "      rc_inds = np.searchsorted(rc, recall_thresholds, side=\"left\")\n",
        "      precision = np.zeros(rc_inds.shape)\n",
        "      is_valid_rc_ind = rc_inds < len(pr)\n",
        "      precision[is_valid_rc_ind] = pr[rc_inds[is_valid_rc_ind]]\n",
        "\n",
        "      precisions.append(precision)\n",
        "      recalls.append(recall)\n",
        "\n",
        "  precisions = np.array(precisions)\n",
        "  recalls = np.array(recalls)\n",
        "\n",
        "  AP = precisions.mean(\n",
        "      axis=1\n",
        "  )  # AP = average precision over fixed set of recall thresholds\n",
        "  AR = recalls  # AR = max recall given a fixed number of detections per image\n",
        "\n",
        "  mAP = precisions.mean()  # mAP = mean over all OKS thresholds\n",
        "  mAR = recalls.mean()  # mAR = mean over all OKS thresholds\n",
        "\n",
        "  return precisions, AP, mAP, mAR"
      ],
      "metadata": {
        "id": "ODoDj8su13FO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Evaluation Code"
      ],
      "metadata": {
        "id": "uaRfJAibv_dz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Get all y_preds and y_gt.\n",
        "xv, yv = make_grid_vectors(\n",
        "    image_height=160,\n",
        "    image_width=160,\n",
        "    output_stride=2\n",
        ")\n",
        "\n",
        "all_y_preds, all_y_gt = [], []\n",
        "for step, ex in enumerate(val_p.make_dataset()):\n",
        "  y_preds = model(ex[\"instance_image\"])\n",
        "  y_preds = find_peaks(y_preds.numpy(), xv, yv)\n",
        "  y_preds = clip_peaks(y_preds)\n",
        "  y_gt = find_peaks(ex[\"instance_confidence_maps\"].numpy(), xv, yv)\n",
        "\n",
        "  all_y_preds.append(y_preds)\n",
        "  all_y_gt.append(y_gt)\n",
        "\n",
        "all_y_preds = np.concatenate(all_y_preds, axis=0)\n",
        "all_y_gt = np.concatenate(all_y_gt, axis=0)"
      ],
      "metadata": {
        "id": "zpaQGxBctxVL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Define thresholds.\n",
        "match_score_thresholds = np.linspace(0.5, 0.95, 10)\n",
        "recall_thresholds = np.linspace(0, 1, 101)\n",
        "\n",
        "# Get evaluation metrics.\n",
        "match_scores = get_match_scores(all_y_preds, all_y_gt)\n",
        "precisions, AP, mAP, mAR = evaluate(match_scores, 400, 0, recall_thresholds, match_score_thresholds)"
      ],
      "metadata": {
        "id": "T1PeMYSVxH4t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mAP, AP"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3bf8pdFkzOFR",
        "outputId": "5ec942d1-cbc4-450f-9c4d-46f6b864224e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(0.8722750563003294,\n",
              " array([0.99721919, 0.99610686, 0.99242706, 0.98310167, 0.97586516,\n",
              "        0.96117723, 0.93709047, 0.87867787, 0.73393265, 0.2671524 ]))"
            ]
          },
          "metadata": {},
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "np.linspace(0.5, 0.95, 10)"
      ],
      "metadata": {
        "id": "5hJKhcNh164U",
        "outputId": "28c2b5b3-b15b-459c-cbab-254527bd4f5b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0.5 , 0.55, 0.6 , 0.65, 0.7 , 0.75, 0.8 , 0.85, 0.9 , 0.95])"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "O4DJHY7ejd3d"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}