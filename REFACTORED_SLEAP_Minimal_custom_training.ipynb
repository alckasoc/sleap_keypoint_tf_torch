{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BeeqrLbdupmE"
      },
      "source": [
        "# Install SLEAP\n",
        "Don't forget to set **Runtime** -> **Change runtime type...** -> **GPU** as the accelerator."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BYxJ2rJOMW8B",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4267e81f-b2c2-48fb-a084-b444f0552e8c"
      },
      "source": [
        "!pip -q install sleap"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m64.4/64.4 MB\u001b[0m \u001b[31m12.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m131.9/131.9 kB\u001b[0m \u001b[31m16.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.9/84.9 kB\u001b[0m \u001b[31m11.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.5/60.5 MB\u001b[0m \u001b[31m12.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m149.2/149.2 kB\u001b[0m \u001b[31m17.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.9/43.9 MB\u001b[0m \u001b[31m16.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.3/3.3 MB\u001b[0m \u001b[31m20.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m88.2/88.2 kB\u001b[0m \u001b[31m9.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m228.9/228.9 kB\u001b[0m \u001b[31m22.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m156.1/156.1 MB\u001b[0m \u001b[31m8.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m214.3/214.3 kB\u001b[0m \u001b[31m23.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.5/4.5 MB\u001b[0m \u001b[31m101.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m498.1/498.1 MB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.3/2.3 MB\u001b[0m \u001b[31m93.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.6/60.6 kB\u001b[0m \u001b[31m7.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m78.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m904.1/904.1 kB\u001b[0m \u001b[31m68.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m26.5/26.5 MB\u001b[0m \u001b[31m60.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m51.1/51.1 kB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.7/50.7 kB\u001b[0m \u001b[31m6.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m832.9/832.9 kB\u001b[0m \u001b[31m61.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m462.3/462.3 kB\u001b[0m \u001b[31m41.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.8/5.8 MB\u001b[0m \u001b[31m101.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.6/42.6 kB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m65.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.4/1.4 MB\u001b[0m \u001b[31m79.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m192.2/192.2 kB\u001b[0m \u001b[31m20.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m109.5/109.5 kB\u001b[0m \u001b[31m15.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.9/4.9 MB\u001b[0m \u001b[31m91.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m485.6/485.6 kB\u001b[0m \u001b[31m32.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for pykalman (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for jsmin (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "tensorflow-metadata 1.13.1 requires protobuf<5,>=3.20.3, but you have protobuf 3.19.6 which is incompatible.\n",
            "flax 0.6.9 requires rich>=11.1, but you have rich 10.16.1 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jftAOyvvuQeh",
        "outputId": "cb559024-d356-4b18-c3b7-e91396198991"
      },
      "source": [
        "import sleap\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import time\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "!pip install albumentations -qqq\n",
        "import albumentations as A\n",
        "\n",
        "sleap.versions()"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "SLEAP: 1.3.0\n",
            "TensorFlow: 2.8.4\n",
            "Numpy: 1.22.4\n",
            "Python: 3.10.11\n",
            "OS: Linux-5.10.147+-x86_64-with-glibc2.31\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wSdTJYOdu4L6"
      },
      "source": [
        "# Download training data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sDIF3RKdM86u",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a51fdfcb-e08e-44b5-d6c4-187510f504ab"
      },
      "source": [
        "!curl -L --output labels.slp https://www.dropbox.com/s/b990gxjt3d3j3jh/210205.sleap_wt_gold.13pt.pkg.slp?dl=1\n",
        "!ls -lah"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "100    17    0    17    0     0     60      0 --:--:-- --:--:-- --:--:--    59\n",
            "100   363  100   363    0     0    460      0 --:--:-- --:--:-- --:--:--   460\n",
            "100  773M  100  773M    0     0  80.7M      0  0:00:09  0:00:09 --:--:-- 95.4M\n",
            "total 774M\n",
            "drwxr-xr-x 1 root root 4.0K Apr 29 11:06 .\n",
            "drwxr-xr-x 1 root root 4.0K Apr 29 11:02 ..\n",
            "drwxr-xr-x 4 root root 4.0K Apr 27 13:34 .config\n",
            "-rw-r--r-- 1 root root 774M Apr 29 11:06 labels.slp\n",
            "drwxr-xr-x 1 root root 4.0K Apr 27 13:35 sample_data\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5lugn7a_HC0Q"
      },
      "source": [
        "# Load the training data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UMkSIZrTHCMr",
        "outputId": "f0b6156f-2dc9-47f2-bd08-cc6b2beeea35"
      },
      "source": [
        "# SLEAP Labels files (.slp) can include the images as well as labeled instances and\n",
        "# other metadata for a project.\n",
        "labels = sleap.load_file(\"labels.slp\")\n",
        "labels = labels.with_user_labels_only()\n",
        "labels.describe()"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Skeleton: Skeleton(description=None, nodes=[head, thorax, abdomen, wingL, wingR, forelegL4, forelegR4, midlegL4, midlegR4, hindlegL4, hindlegR4, eyeL, eyeR], edges=[thorax->head, thorax->abdomen, thorax->wingL, thorax->wingR, thorax->forelegL4, thorax->forelegR4, thorax->midlegL4, thorax->midlegR4, thorax->hindlegL4, thorax->hindlegR4, head->eyeL, head->eyeR], symmetries=[eyeL<->eyeR, wingL<->wingR, hindlegL4<->hindlegR4, forelegL4<->forelegR4, midlegL4<->midlegR4])\n",
            "Videos: ['labels.slp', 'labels.slp', 'labels.slp', 'labels.slp', 'labels.slp', 'labels.slp', 'labels.slp', 'labels.slp', 'labels.slp', 'labels.slp', 'labels.slp', 'labels.slp', 'labels.slp', 'labels.slp', 'labels.slp', 'labels.slp', 'labels.slp', 'labels.slp', 'labels.slp', 'labels.slp', 'labels.slp', 'labels.slp', 'labels.slp', 'labels.slp', 'labels.slp', 'labels.slp', 'labels.slp', 'labels.slp', 'labels.slp', 'labels.slp']\n",
            "Frames (user/predicted): 2,000/0\n",
            "Instances (user/predicted): 4,000/0\n",
            "Tracks: [Track(spawned_on=0, name='female'), Track(spawned_on=0, name='male')]\n",
            "Suggestions: 3,000\n",
            "Provenance: {}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EK8DDPHDHBr_",
        "outputId": "ed30ef02-83d7-47b0-8a70-100e8cff4520"
      },
      "source": [
        "# Labels are list-like containers whose elements are LabeledFrames\n",
        "print(f\"Number of labels: {len(labels)}\")\n",
        "\n",
        "labeled_frame = labels[0]\n",
        "labeled_frame"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of labels: 2000\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "LabeledFrame(video=HDF5Video('labels.slp'), frame_idx=7455, instances=2)"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pP5uXrU3HX6o",
        "outputId": "6801c964-c8b4-424e-85ae-e4f253339e92"
      },
      "source": [
        "# LabeledFrames are containers for instances that were labeled in a single frame\n",
        "instance = labeled_frame[0]\n",
        "instance"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Instance(video=Video(filename=labels.slp, shape=(55, 1024, 1024, 1), backend=HDF5Video), frame_idx=7455, points=[head: (633.5, 806.0), thorax: (589.2, 807.0), abdomen: (557.2, 813.5), wingL: (538.7, 812.6), wingR: (538.9, 814.8), forelegL4: (652.0, 797.0), forelegR4: (655.2, 814.1), midlegL4: (611.5, 779.4), midlegR4: (614.1, 862.1), hindlegL4: (540.4, 778.7), hindlegR4: (570.8, 843.3), eyeL: (621.4, 795.1), eyeR: (622.4, 816.8)], track=Track(spawned_on=0, name='female'))"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "700fTPs4Hurg",
        "outputId": "373c3fad-f469-42c8-889b-6a2abd5f4265"
      },
      "source": [
        "# They can be converted to numpy arrays where each row corresponds to the coordinates\n",
        "# of a different body part:\n",
        "pts = instance.numpy()\n",
        "pts"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "rec.array([[633.53948229, 806.03863528],\n",
              "           [589.19035306, 807.03863528],\n",
              "           [557.15087078, 813.50056467],\n",
              "           [538.69299954, 812.58872353],\n",
              "           [538.94660133, 814.78180105],\n",
              "           [652.00028234, 797.03863528],\n",
              "           [655.24461549, 814.09569239],\n",
              "           [611.5       , 779.38494116],\n",
              "           [614.08309628, 862.14672415],\n",
              "           [540.41406264, 778.65699169],\n",
              "           [570.76953595, 843.28358376],\n",
              "           [621.39804036, 795.0546654 ],\n",
              "           [622.35635663, 816.77982951]],\n",
              "          dtype=float64)"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-vYsPusvviiu"
      },
      "source": [
        "# Setup training data generation\n",
        "\n",
        "For a top-down pipeline, the subnetwork (*centered instance model*) that takes cropped images as input and produces pose estimates as output requires **confidence maps** to train against as targets.\n",
        "\n",
        "Here we will create a pipeline that does the sampling, augmentation, data generation and\n",
        "batching so we can create our input/output pairs."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def update_not_shown_nodes(not_shown_nodes, node_names, new_nodes):\n",
        "  nodes_not_in_aug = np.array(list(set(node_names).difference(set(new_nodes))))\n",
        "  not_shown_in_aug_or_original_ind = np.in1d(node_names, nodes_not_in_aug).nonzero()[0]\n",
        "  not_shown_nodes[not_shown_in_aug_or_original_ind] = True \n",
        "\n",
        "  return not_shown_nodes\n",
        "\n",
        "\n",
        "def update_kp(kp, not_shown_nodes, node_names, new_nodes):\n",
        "  shown_after_aug_ind = np.in1d(node_names, new_nodes).nonzero()[0]\n",
        "  shown_ind = np.in1d(not_shown_nodes, True).nonzero()[0]\n",
        "\n",
        "  assert len(shown_after_aug_ind) == kp.shape[0]\n",
        "\n",
        "  kp_ = np.zeros((len(node_names), 2))\n",
        "  kp_[shown_after_aug_ind] = kp\n",
        "  kp_[shown_ind] = 0\n",
        "\n",
        "  return kp_\n",
        "\n",
        "\n",
        "def make_grid_vectors(\n",
        "    image_height: int, image_width: int, output_stride: int = 1):\n",
        "\n",
        "    xv = torch.arange(0, image_width, step=output_stride).to(torch.float32)\n",
        "    yv = torch.arange(0, image_height, step=output_stride).to(torch.float32)\n",
        "    return xv, yv\n",
        "\n",
        "def make_confmaps(\n",
        "    points: torch.Tensor, xv: torch.Tensor, yv: torch.Tensor, sigma: float):\n",
        "\n",
        "    x = torch.reshape(points[:, 0], (1, 1, -1))\n",
        "    y = torch.reshape(points[:, 1], (1, 1, -1))\n",
        "    cm = torch.exp(\n",
        "        -((torch.reshape(xv, (1, -1, 1)) - x) ** 2 + (torch.reshape(yv, (-1, 1, 1)) - y) ** 2)\n",
        "        / (2 * sigma ** 2)\n",
        "    )\n",
        "\n",
        "    # Replace NaNs with 0.\n",
        "    cm = torch.where(torch.isnan(cm), 0.0, cm)\n",
        "    return cm\n",
        "\n",
        "def get_bbox_coords_on_centroid(anchor_coords, crop_size, img_size):\n",
        "    (cx, cy) = anchor_coords\n",
        "\n",
        "    # [bottom left     top right]\n",
        "    # [  x1, y1,         x2, y2 ]\n",
        "    bbox = [\n",
        "        max(-crop_size / 2 + cx, 0),\n",
        "        max(-crop_size / 2 + cy, 0),\n",
        "        min(crop_size / 2 + cx, img_size[0]),\n",
        "        min(crop_size / 2 + cy, img_size[1])\n",
        "    ]\n",
        "\n",
        "    return bbox\n",
        "\n",
        "# My refactored version of this dataset generator.\n",
        "class DataGenerator(Dataset):\n",
        "    def __init__(self, \n",
        "      labels, \n",
        "      img_size=160,\n",
        "      anchor_name=\"thorax\",\n",
        "      sigma=1.5,\n",
        "      output_stride=2,\n",
        "      rot_range=(-180, 180)\n",
        "    ):\n",
        "        self.labels = labels.with_user_labels_only()\n",
        "        self.labels.remove_empty_instances(keep_empty_frames=False)\n",
        "\n",
        "        self.indices = []\n",
        "        for frame_idx, l in enumerate(self.labels):\n",
        "          inst_indices = np.arange(0, len(l.instances)).tolist()\n",
        "          self.indices.extend([(frame_idx, i) for i in inst_indices])\n",
        "\n",
        "        self.img_size = img_size\n",
        "\n",
        "        assert anchor_name in self.labels.skeleton.node_names\n",
        "        self.anchor_name = anchor_name\n",
        "\n",
        "        # Assuming 1 skeleton.\n",
        "        assert len(labels.skeletons) == 1\n",
        "        self.node_names = labels.skeletons[0].node_names\n",
        "\n",
        "        self.sigma = sigma\n",
        "        self.output_stride = output_stride\n",
        "        self.rot_range = rot_range\n",
        "\n",
        "        self.tfm = A.Compose([\n",
        "            A.Rotate(limit=list(self.rot_range), p=0.5)\n",
        "        ], keypoint_params=A.KeypointParams(format='xy', label_fields=['class_labels']))\n",
        "\n",
        "        self.xv, self.yv = make_grid_vectors(\n",
        "            image_height=self.img_size, \n",
        "            image_width=self.img_size, \n",
        "            output_stride=self.output_stride\n",
        "        )\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.indices)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        frame_idx, inst_idx = self.indices[idx]\n",
        "        lf = self.labels[frame_idx]\n",
        "        instance = lf[inst_idx]\n",
        "        img = lf.image\n",
        "        kp = instance.numpy()\n",
        "\n",
        "        # NaNs to 0 and clip.\n",
        "        assert kp.shape == (len(self.node_names), 2)\n",
        "        not_shown_nodes = np.isnan(kp).any(axis=1)\n",
        "        kp = np.nan_to_num(kp, nan=0)\n",
        "        kp = np.concatenate((np.clip(kp[:, :1], 0, img.shape[1]), \n",
        "                              np.clip(kp[:, 1:], 0, img.shape[0])), \n",
        "                            axis=1)\n",
        "\n",
        "        # Apply augmentations.\n",
        "        output = self.tfm(image=img, keypoints=kp, class_labels=self.node_names)\n",
        "        img, kp, new_nodes = output[\"image\"], np.array(output[\"keypoints\"]), output[\"class_labels\"]\n",
        "\n",
        "        # Update not_shown_nodes and kp.\n",
        "        not_shown_nodes = update_not_shown_nodes(not_shown_nodes, self.node_names, new_nodes)\n",
        "        kp = update_kp(kp, not_shown_nodes, self.node_names, new_nodes)\n",
        "\n",
        "        # Get bbox coordinate based on centroid.\n",
        "        bbox = get_bbox_coords_on_centroid(\n",
        "          kp[self.node_names.index(self.anchor_name)].tolist(), \n",
        "          self.img_size, img.shape[:2]\n",
        "        )\n",
        "\n",
        "        # Crop and pad.\n",
        "        x1, y1, x2, y2 = bbox\n",
        "        tfm_crop = A.Compose([\n",
        "          A.Crop(int(round(x1)), int(round(y1)), int(round(x2)), int(round(y2))),\n",
        "          A.PadIfNeeded(min_height=self.img_size, min_width=self.img_size)\n",
        "        ], keypoint_params=A.KeypointParams(format='xy', label_fields=['class_labels']))\n",
        "\n",
        "        crop_kp = tfm_crop(image=img, keypoints=kp, class_labels=self.node_names)\n",
        "        crop, kp, new_nodes = crop_kp[\"image\"], np.array(crop_kp[\"keypoints\"]), crop_kp[\"class_labels\"]\n",
        "        crop = torch.Tensor(crop).permute(2, 0, 1)\n",
        "\n",
        "        # Update not_shown_nodes and kp.\n",
        "        not_shown_nodes = update_not_shown_nodes(not_shown_nodes, self.node_names, new_nodes)\n",
        "        kp = update_kp(kp, not_shown_nodes, self.node_names, new_nodes)\n",
        "        kp = torch.Tensor(kp)\n",
        "\n",
        "        # Get confidence map.\n",
        "        xv, yv = make_grid_vectors(\n",
        "          image_height=self.img_size, \n",
        "          image_width=self.img_size, \n",
        "          output_stride=self.output_stride\n",
        "        )\n",
        "\n",
        "        cm = make_confmaps(\n",
        "          points=kp, \n",
        "          xv=self.xv, \n",
        "          yv=self.yv, \n",
        "          sigma=self.sigma\n",
        "        )\n",
        "        cm = cm.permute(2, 0, 1)\n",
        "\n",
        "        return crop, cm\n",
        "\n",
        "ds = DataGenerator(labels)"
      ],
      "metadata": {
        "id": "l8Q-1ehtVWbr"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0yo95XzRKkba"
      },
      "source": [
        "# Setting up a neural network model\n",
        "\n",
        "Here we'll create a simple UNet CNN that is compatible with the input and output shapes\n",
        "that we're generating."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "import numpy as np\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch.nn import functional as F\n",
        "import collections\n",
        "from typing import Sequence, Tuple, Text, Union, Optional, List\n",
        "\n",
        "class MaxPool2dWithSamePadding(nn.MaxPool2d):\n",
        "\n",
        "    def _calc_same_pad(self, i: int, k: int, s: int, d: int) -> int:\n",
        "        return max((math.ceil(i / s) - 1) * s + (k - 1) * d + 1 - i, 0)\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        if self.padding == \"same\":\n",
        "            ih, iw = x.size()[-2:]\n",
        "\n",
        "            pad_h = self._calc_same_pad(i=ih, \n",
        "                                        k=self.kernel_size if type(self.kernel_size) is int else self.kernel_size[0], \n",
        "                                        s=self.stride if type(self.stride) is int else self.stride[0], \n",
        "                                        d=self.dilation if type(self.dilation) is int else self.dilation[0])\n",
        "            pad_w = self._calc_same_pad(i=iw, \n",
        "                                        k=self.kernel_size if type(self.kernel_size) is int else self.kernel_size[1], \n",
        "                                        s=self.stride if type(self.stride) is int else self.stride[1], \n",
        "                                        d=self.dilation if type(self.dilation) is int else self.dilation[1])\n",
        "\n",
        "            if pad_h > 0 or pad_w > 0:\n",
        "                x = F.pad(\n",
        "                    x, [pad_w // 2, pad_w - pad_w // 2, pad_h // 2, pad_h - pad_h // 2]\n",
        "                )\n",
        "            self.padding = 0\n",
        "\n",
        "        return F.max_pool2d(x, self.kernel_size, self.stride,\n",
        "                                    self.padding, self.dilation, ceil_mode=self.ceil_mode,\n",
        "                                    return_indices=self.return_indices)\n",
        "\n",
        "def get_act_fn(activation: str) -> nn.Module:\n",
        "    activations = {\n",
        "        'relu': nn.ReLU(),\n",
        "        'sigmoid': nn.Sigmoid(),\n",
        "        'tanh': nn.Tanh()\n",
        "    }\n",
        "\n",
        "    return activations[activation]\n",
        "\n",
        "class SimpleConvBlock(nn.Module):\n",
        "    def __init__(self,\n",
        "        in_channels: int,\n",
        "        pool: bool = True,\n",
        "        pooling_stride: int = 2,\n",
        "        pool_before_convs: bool = False,\n",
        "        num_convs: int = 2,\n",
        "        filters: int = 32,\n",
        "        kernel_size: int = 3,\n",
        "        use_bias: bool = True,\n",
        "        batch_norm: bool = False,\n",
        "        activation: Text = \"relu\"\n",
        "    ) -> None:\n",
        "        super().__init__()\n",
        "\n",
        "        self.in_channels = in_channels\n",
        "        self.pool = pool\n",
        "        self.pooling_stride = pooling_stride\n",
        "        self.pool_before_convs = pool_before_convs\n",
        "        self.num_convs = num_convs\n",
        "        self.filters = filters\n",
        "        self.kernel_size = kernel_size\n",
        "        self.use_bias = use_bias\n",
        "        self.batch_norm = batch_norm\n",
        "        self.activation = activation\n",
        "\n",
        "        self.blocks = []\n",
        "        if pool and pool_before_convs:\n",
        "            self.blocks.append(\n",
        "                MaxPool2dWithSamePadding(\n",
        "                    kernel_size=2,\n",
        "                    stride=pooling_stride,\n",
        "                    padding=\"same\"\n",
        "                )\n",
        "            )\n",
        "\n",
        "        for i in range(num_convs):\n",
        "            self.blocks.append(\n",
        "                nn.Conv2d(\n",
        "                    in_channels=in_channels if i == 0 else filters,\n",
        "                    out_channels=filters,\n",
        "                    kernel_size=kernel_size,\n",
        "                    stride=1,\n",
        "                    padding=\"same\",\n",
        "                    bias=use_bias\n",
        "                )\n",
        "            )\n",
        "\n",
        "            if batch_norm:\n",
        "                self.blocks.append(\n",
        "                    nn.BatchNorm2d(filters)\n",
        "                )\n",
        "\n",
        "            self.blocks.append(\n",
        "                get_act_fn(activation)  \n",
        "            )\n",
        "\n",
        "\n",
        "        if pool and not pool_before_convs:\n",
        "            self.blocks.append(\n",
        "                MaxPool2dWithSamePadding(\n",
        "                    kernel_size=2,\n",
        "                    stride=pooling_stride,\n",
        "                    padding=\"same\"\n",
        "                )\n",
        "            )\n",
        "\n",
        "        self.blocks = nn.Sequential(*self.blocks)\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        return self.blocks(x)\n",
        "\n",
        "class Encoder(nn.Module):\n",
        "    def __init__(self,\n",
        "        in_channels: int = 3,\n",
        "        filters: int = 64,\n",
        "        down_blocks: int = 4, \n",
        "        filters_rate: Union[float, int] = 2,\n",
        "        current_stride: int = 2,\n",
        "        stem_blocks: int = 0,\n",
        "        convs_per_block: int = 2,\n",
        "        kernel_size: Union[int, Tuple[int, int]] = 3,\n",
        "        middle_block: bool = True,\n",
        "        block_contraction: bool = False\n",
        "    ) -> None:\n",
        "        super().__init__()\n",
        "\n",
        "        self.in_channels = in_channels\n",
        "        self.filters = filters\n",
        "        self.down_blocks = down_blocks\n",
        "        self.filters_rate = filters_rate\n",
        "        self.current_stride = current_stride\n",
        "        self.stem_blocks = stem_blocks\n",
        "        self.convs_per_block = convs_per_block\n",
        "        self.kernel_size = kernel_size\n",
        "        self.middle_block = middle_block\n",
        "        self.block_contraction = block_contraction\n",
        "\n",
        "        self.encoder_stack = nn.ModuleList([])\n",
        "        for block in range(down_blocks):\n",
        "            prev_block_filters = -1 if block==0 else block_filters\n",
        "            block_filters = int(\n",
        "                filters * (filters_rate ** (block + stem_blocks))\n",
        "            )\n",
        "\n",
        "            self.encoder_stack.append(\n",
        "                SimpleConvBlock(\n",
        "                    in_channels=in_channels if block == 0 else prev_block_filters,\n",
        "                    pool=(block > 0),\n",
        "                    pool_before_convs=True,\n",
        "                    pooling_stride=2,\n",
        "                    num_convs=convs_per_block,\n",
        "                    filters=block_filters,\n",
        "                    kernel_size=kernel_size,\n",
        "                    use_bias=True,\n",
        "                    batch_norm=False,\n",
        "                    activation=\"relu\"\n",
        "                )\n",
        "            )\n",
        "        after_block_filters = block_filters\n",
        "\n",
        "        self.encoder_stack.append(\n",
        "            MaxPool2dWithSamePadding(\n",
        "                kernel_size=2,\n",
        "                stride=2,\n",
        "                padding=\"same\"\n",
        "            )\n",
        "        )\n",
        "\n",
        "        # Create a middle block (like the CARE implementation).\n",
        "        if middle_block:\n",
        "            if convs_per_block > 1:\n",
        "                # First convs are one exponent higher than the last encoder block.\n",
        "                block_filters = int(\n",
        "                    filters * (filters_rate ** (down_blocks + stem_blocks))\n",
        "                )\n",
        "                self.encoder_stack.append(\n",
        "                    SimpleConvBlock(\n",
        "                        in_channels=after_block_filters,\n",
        "                        pool=False,\n",
        "                        pool_before_convs=False,\n",
        "                        pooling_stride=2,\n",
        "                        num_convs=convs_per_block - 1,\n",
        "                        filters=block_filters,\n",
        "                        kernel_size=kernel_size,\n",
        "                        use_bias=True,\n",
        "                        batch_norm=False,\n",
        "                        activation=\"relu\",\n",
        "                    )\n",
        "                )\n",
        "\n",
        "            if block_contraction:\n",
        "                # Contract the channels with an exponent lower than the last encoder block.\n",
        "                block_filters = int(\n",
        "                    filters * (filters_rate ** (down_blocks + stem_blocks - 1))\n",
        "                )\n",
        "            else:\n",
        "                # Keep the block output filters the same.\n",
        "                block_filters = int(\n",
        "                    filters * (filters_rate ** (down_blocks + stem_blocks))\n",
        "                )\n",
        "\n",
        "            self.encoder_stack.append(\n",
        "                SimpleConvBlock(\n",
        "                    in_channels=block_filters,\n",
        "                    pool=False,\n",
        "                    pool_before_convs=False,\n",
        "                    pooling_stride=2,\n",
        "                    num_convs=1,\n",
        "                    filters=block_filters,\n",
        "                    kernel_size=kernel_size,\n",
        "                    use_bias=True,\n",
        "                    batch_norm=False,\n",
        "                    activation=\"relu\",\n",
        "                )\n",
        "            )\n",
        "\n",
        "        self.intermediate_features = {}\n",
        "        for i, block in enumerate(self.encoder_stack):\n",
        "            if isinstance(block, SimpleConvBlock) and block.pool:\n",
        "                current_stride *= block.pooling_stride\n",
        "\n",
        "            if current_stride not in self.intermediate_features.values():\n",
        "                self.intermediate_features[i] = current_stride\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        features = []\n",
        "        for i in range(len(self.encoder_stack)):\n",
        "            x = self.encoder_stack[i](x)\n",
        "            \n",
        "            if i in self.intermediate_features.keys():\n",
        "                features.append(x)\n",
        "\n",
        "        return x, features[1:][::-1]\n",
        "\n",
        "class SimpleUpsamplingBlock(nn.Module):\n",
        "    def __init__(self,\n",
        "        x_in_shape: int, \n",
        "        current_stride: int,\n",
        "        upsampling_stride: int = 2,\n",
        "        interp_method: Text = \"bilinear\",\n",
        "        refine_convs: int = 2,\n",
        "        refine_convs_filters: int = 64,\n",
        "        refine_convs_kernel_size: int = 3,\n",
        "        refine_convs_use_bias: bool = True,\n",
        "        refine_convs_batch_norm: bool = True,\n",
        "        refine_convs_batch_norm_before_activation: bool = True,\n",
        "        refine_convs_activation: Text = \"relu\"\n",
        "    ) -> None:\n",
        "        super().__init__()\n",
        "\n",
        "        self.x_in_shape = x_in_shape\n",
        "        self.current_stride = current_stride\n",
        "        self.upsampling_stride = upsampling_stride\n",
        "        self.interp_method = interp_method\n",
        "        self.refine_convs = refine_convs\n",
        "        self.refine_convs_filters = refine_convs_filters\n",
        "        self.refine_convs_kernel_size = refine_convs_kernel_size\n",
        "        self.refine_convs_use_bias = refine_convs_use_bias\n",
        "        self.refine_convs_batch_norm = refine_convs_batch_norm\n",
        "        self.refine_convs_batch_norm_before_activation = refine_convs_batch_norm_before_activation\n",
        "        self.refine_convs_activation = refine_convs_activation\n",
        "\n",
        "        self.blocks = nn.ModuleList([])\n",
        "        if current_stride is not None:\n",
        "            # Append the strides to the block prefix.\n",
        "            new_stride = current_stride // upsampling_stride\n",
        "\n",
        "        # Upsample via interpolation.\n",
        "        self.blocks.append(\n",
        "            nn.Upsample(\n",
        "                scale_factor=upsampling_stride,\n",
        "                mode=interp_method,\n",
        "            )\n",
        "        )\n",
        "\n",
        "\n",
        "        # Add further convolutions to refine after upsampling and/or skip.\n",
        "        for i in range(refine_convs):\n",
        "            filters = refine_convs_filters\n",
        "            self.blocks.append(\n",
        "                nn.Conv2d(\n",
        "                    in_channels=x_in_shape if i==0 else filters,\n",
        "                    out_channels=filters,\n",
        "                    kernel_size=refine_convs_kernel_size,\n",
        "                    stride=1,\n",
        "                    padding=\"same\",\n",
        "                    bias=refine_convs_use_bias\n",
        "                )\n",
        "            )\n",
        "\n",
        "            if (\n",
        "                refine_convs_batch_norm\n",
        "                and refine_convs_batch_norm_before_activation\n",
        "            ):\n",
        "                self.blocks.append(nn.BatchNorm2d(num_features=filters))\n",
        "\n",
        "\n",
        "            self.blocks.append(\n",
        "                get_act_fn(refine_convs_activation)  \n",
        "            )\n",
        "\n",
        "            if (\n",
        "                refine_convs_batch_norm\n",
        "                and not refine_convs_batch_norm_before_activation\n",
        "            ):\n",
        "                self.blocks.append(nn.BatchNorm2d(num_features=filters))\n",
        "\n",
        "\n",
        "    def forward(self, x: torch.Tensor, feature: torch.Tensor) -> torch.Tensor:\n",
        "        for idx, b in enumerate(self.blocks):\n",
        "            if idx == 1:  # Right after upsampling or convtranspose2d.\n",
        "                x = torch.concat((x, feature), dim=1)\n",
        "            x = b(x)\n",
        "\n",
        "        return x\n",
        "\n",
        "class Decoder(nn.Module):\n",
        "    def __init__(self,\n",
        "        x_in_shape: int,\n",
        "        current_stride: int,\n",
        "        filters: int = 64,\n",
        "        up_blocks: int = 4,\n",
        "        down_blocks: int = 3,\n",
        "        filters_rate: int = 2,\n",
        "        stem_blocks: int = 0,\n",
        "        convs_per_block: int = 2,\n",
        "        kernel_size: int = 3,\n",
        "        block_contraction: bool = False\n",
        "    ) -> None:\n",
        "        super().__init__()\n",
        "\n",
        "        self.x_in_shape = x_in_shape\n",
        "        self.current_stride = current_stride\n",
        "        self.filters = filters\n",
        "        self.up_blocks = up_blocks\n",
        "        self.down_blocks = down_blocks\n",
        "        self.filters_rate = filters_rate\n",
        "        self.stem_blocks = stem_blocks\n",
        "        self.convs_per_block = convs_per_block\n",
        "        self.kernel_size = kernel_size\n",
        "        self.block_contraction = block_contraction\n",
        "\n",
        "        self.decoder_stack = nn.ModuleList([])\n",
        "        for block in range(up_blocks):\n",
        "            prev_block_filters_in = -1 if block == 0 else block_filters_in\n",
        "            block_filters_in = int(\n",
        "                filters\n",
        "                * (\n",
        "                    filters_rate\n",
        "                    ** (down_blocks + stem_blocks - 1 - block)\n",
        "                )\n",
        "            )\n",
        "            if block_contraction:\n",
        "                block_filters_out = int(\n",
        "                    filters\n",
        "                    * (\n",
        "                        filters_rate\n",
        "                        ** (down_blocks + stem_blocks - 2 - block)\n",
        "                    )\n",
        "                )\n",
        "            else:\n",
        "                block_filters_out = block_filters_in\n",
        "\n",
        "            next_stride = current_stride // 2\n",
        "\n",
        "            self.decoder_stack.append(\n",
        "                SimpleUpsamplingBlock(\n",
        "                    x_in_shape=(x_in_shape + block_filters_in) if block == 0 else (prev_block_filters_in + block_filters_in), \n",
        "                    current_stride=current_stride,\n",
        "                    upsampling_stride=2,\n",
        "                    interp_method=\"bilinear\",\n",
        "                    refine_convs=self.convs_per_block,\n",
        "                    refine_convs_filters=block_filters_out,\n",
        "                    refine_convs_kernel_size=self.kernel_size,\n",
        "                    refine_convs_batch_norm=False,\n",
        "                )\n",
        "            )\n",
        "\n",
        "            current_stride = next_stride\n",
        "\n",
        "    def forward(self, x: torch.Tensor, features: List[torch.Tensor]) -> torch.Tensor:\n",
        "        for i in range(len(self.decoder_stack)):\n",
        "            x = self.decoder_stack[i](x, features[i])\n",
        "\n",
        "        return x\n",
        "\n",
        "class Unet(nn.Module):\n",
        "    def __init__(self,\n",
        "        in_channels: int = 1,\n",
        "        kernel_size: int = 3,\n",
        "        filters: int = 32,\n",
        "        filters_rate: int = 1.5,\n",
        "        stem_blocks: int = 0,\n",
        "        down_blocks: int = 4, \n",
        "        up_blocks: int = 3,\n",
        "        convs_per_block: int = 2,\n",
        "        middle_block: bool = True,\n",
        "        block_contraction: bool = False\n",
        "    ) -> None:\n",
        "        super().__init__()\n",
        "\n",
        "        self.enc = Encoder(\n",
        "            in_channels=in_channels,\n",
        "            filters=filters,\n",
        "            down_blocks=down_blocks,\n",
        "            filters_rate=filters_rate,\n",
        "            stem_blocks=stem_blocks,\n",
        "            convs_per_block=convs_per_block,\n",
        "            kernel_size=kernel_size,\n",
        "            middle_block=middle_block,\n",
        "            block_contraction=block_contraction\n",
        "        )\n",
        "\n",
        "        current_stride = int(\n",
        "            np.prod(\n",
        "                [block.pooling_stride for block in self.enc.encoder_stack if hasattr(block, \"pool\") and block.pool]\n",
        "                + [1]\n",
        "            )\n",
        "        )\n",
        "        \n",
        "        x_in_shape = int(\n",
        "            filters * (filters_rate ** (down_blocks + stem_blocks))\n",
        "        )\n",
        "\n",
        "        self.dec = Decoder(x_in_shape=x_in_shape, current_stride=current_stride, filters=filters,\n",
        "            up_blocks=up_blocks, down_blocks=down_blocks, filters_rate=filters_rate\n",
        "        )\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        x, features = self.enc(x)\n",
        "        x = self.dec(x, features)\n",
        "        return x\n"
      ],
      "metadata": {
        "id": "fCMnTHR9VetA"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gEfgZT4hLyW4"
      },
      "source": [
        "# Train the model\n",
        "\n",
        "Now we have everything we need to do basic training!\n",
        "\n",
        "Let's set up a custom training loop that trains on each batch of data from our pipeline.\n",
        "\n",
        "See [this guide](https://www.tensorflow.org/guide/keras/writing_a_training_loop_from_scratch) for more info on how to customize the training process in TensorFlow."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "filters = 32\n",
        "filters_rate = 1.5\n",
        "down_blocks = 4\n",
        "stem_blocks = 0\n",
        "up_blocks = 3\n",
        "\n",
        "unet = Unet(filters=filters, \n",
        "            filters_rate=filters_rate, \n",
        "            down_blocks=down_blocks, \n",
        "            stem_blocks=stem_blocks, \n",
        "            up_blocks=up_blocks)\n",
        "\n",
        "in_channels = int(\n",
        "    filters\n",
        "    * (\n",
        "        filters_rate\n",
        "        ** (down_blocks + stem_blocks - 1 - up_blocks + 1)\n",
        "    )\n",
        ")\n",
        "model = nn.Sequential(*[\n",
        "    unet, \n",
        "    nn.Conv2d(in_channels=in_channels, out_channels=13, kernel_size=1, padding=\"same\")    \n",
        "])\n",
        "\n",
        "opt = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
        "scaler = torch.cuda.amp.GradScaler()"
      ],
      "metadata": {
        "id": "QoSLDG5OXWRW"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import multiprocessing\n",
        "\n",
        "cores = multiprocessing.cpu_count()\n",
        "cores"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WFNfmlnprhmb",
        "outputId": "542355d7-66f3-4795-c860-009c52da546a"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_ds = DataGenerator(labels)\n",
        "train_dl = DataLoader(\n",
        "    train_ds,\n",
        "    batch_size=4,\n",
        "    shuffle=True,\n",
        "    num_workers=cores,\n",
        "    pin_memory=True,\n",
        "    drop_last=True,\n",
        "    prefetch_factor=2\n",
        ")"
      ],
      "metadata": {
        "id": "MQgZkAySXpO-"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = model.to(\"cuda\")\n",
        "_ = model.train()\n",
        "\n",
        "for epoch in range(3):\n",
        "  start_time = time.time()\n",
        "\n",
        "  for idx, batch in enumerate(train_dl):\n",
        "    X, y = batch\n",
        "    X = X.to(\"cuda\")\n",
        "    y = y.to(\"cuda\")\n",
        "    \n",
        "    with torch.autocast(\"cuda\"):\n",
        "      y_preds = model(X)\n",
        "      loss = nn.MSELoss()(y_preds, y)\n",
        "    scaler.scale(loss).backward()\n",
        "    scaler.step(opt)\n",
        "    scaler.update()\n",
        "\n",
        "    opt.zero_grad()\n",
        "\n",
        "    if idx % 100 == 0:\n",
        "      print(f\"Epoch: {epoch} | Loss: {loss:.5f}\")\n",
        "  print(\"--- %s seconds ---\" % (time.time() - start_time))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tDMSgbrrY7E0",
        "outputId": "5b3b6566-de50-4225-8cb6-750220821dce"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 0 | Loss: 0.01139\n",
            "Epoch: 0 | Loss: 0.00039\n",
            "Epoch: 0 | Loss: 0.00030\n",
            "Epoch: 0 | Loss: 0.00028\n",
            "Epoch: 0 | Loss: 0.00028\n",
            "Epoch: 0 | Loss: 0.00028\n",
            "Epoch: 0 | Loss: 0.00027\n",
            "Epoch: 0 | Loss: 0.00028\n",
            "Epoch: 0 | Loss: 0.00027\n",
            "Epoch: 0 | Loss: 0.00027\n",
            "--- 88.79755687713623 seconds ---\n",
            "Epoch: 1 | Loss: 0.00027\n",
            "Epoch: 1 | Loss: 0.00027\n",
            "Epoch: 1 | Loss: 0.00027\n",
            "Epoch: 1 | Loss: 0.00027\n",
            "Epoch: 1 | Loss: 0.00027\n",
            "Epoch: 1 | Loss: 0.00027\n",
            "Epoch: 1 | Loss: 0.00027\n",
            "Epoch: 1 | Loss: 0.00026\n",
            "Epoch: 1 | Loss: 0.00027\n",
            "Epoch: 1 | Loss: 0.00027\n",
            "--- 82.58847284317017 seconds ---\n",
            "Epoch: 2 | Loss: 0.00027\n",
            "Epoch: 2 | Loss: 0.00027\n",
            "Epoch: 2 | Loss: 0.00026\n",
            "Epoch: 2 | Loss: 0.00026\n",
            "Epoch: 2 | Loss: 0.00027\n",
            "Epoch: 2 | Loss: 0.00026\n",
            "Epoch: 2 | Loss: 0.00026\n",
            "Epoch: 2 | Loss: 0.00026\n",
            "Epoch: 2 | Loss: 0.00026\n",
            "Epoch: 2 | Loss: 0.00026\n",
            "--- 80.36733961105347 seconds ---\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "l_K2JBrPZQVC"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}