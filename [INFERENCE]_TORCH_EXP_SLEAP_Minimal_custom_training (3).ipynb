{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BeeqrLbdupmE"
      },
      "source": [
        "# Install SLEAP\n",
        "Don't forget to set **Runtime** -> **Change runtime type...** -> **GPU** as the accelerator."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "BYxJ2rJOMW8B",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "50bf8c44-957d-4313-bf71-a34a861262d0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m64.4/64.4 MB\u001b[0m \u001b[31m12.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.6/60.6 kB\u001b[0m \u001b[31m7.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.5/4.5 MB\u001b[0m \u001b[31m70.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m72.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.5/60.5 MB\u001b[0m \u001b[31m9.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.9/84.9 kB\u001b[0m \u001b[31m8.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.3/3.3 MB\u001b[0m \u001b[31m51.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.9/43.9 MB\u001b[0m \u001b[31m9.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m26.5/26.5 MB\u001b[0m \u001b[31m42.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.3/2.3 MB\u001b[0m \u001b[31m38.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m904.1/904.1 kB\u001b[0m \u001b[31m34.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m228.9/228.9 kB\u001b[0m \u001b[31m26.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m214.3/214.3 kB\u001b[0m \u001b[31m18.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m149.2/149.2 kB\u001b[0m \u001b[31m15.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m131.9/131.9 kB\u001b[0m \u001b[31m13.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m88.2/88.2 kB\u001b[0m \u001b[31m10.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m498.1/498.1 MB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m156.1/156.1 MB\u001b[0m \u001b[31m7.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m51.1/51.1 kB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.7/50.7 kB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m832.9/832.9 kB\u001b[0m \u001b[31m70.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.6/42.6 kB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m33.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.8/5.8 MB\u001b[0m \u001b[31m40.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m462.3/462.3 kB\u001b[0m \u001b[31m37.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.4/1.4 MB\u001b[0m \u001b[31m38.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m197.1/197.1 kB\u001b[0m \u001b[31m20.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m112.2/112.2 kB\u001b[0m \u001b[31m13.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.9/4.9 MB\u001b[0m \u001b[31m55.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m781.3/781.3 kB\u001b[0m \u001b[31m63.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m485.6/485.6 kB\u001b[0m \u001b[31m48.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for pykalman (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for jsmin (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "flax 0.6.11 requires rich>=11.1, but you have rich 10.16.1 which is incompatible.\n",
            "tensorflow-datasets 4.9.2 requires protobuf>=3.20, but you have protobuf 3.19.6 which is incompatible.\n",
            "tensorflow-metadata 1.13.1 requires protobuf<5,>=3.20.3, but you have protobuf 3.19.6 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for nvidia-ml-py3 (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ],
      "source": [
        "!pip install sleap -qqq\n",
        "!pip install albumentations -qqq\n",
        "!pip install nvidia-ml-py3 -qqq"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jftAOyvvuQeh",
        "outputId": "9809d9b5-13c9-47d4-8c83-76e83b84524e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "SLEAP: 1.3.1\n",
            "TensorFlow: 2.8.4\n",
            "Numpy: 1.22.4\n",
            "Python: 3.10.12\n",
            "OS: Linux-5.15.107+-x86_64-with-glibc2.31\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import gc\n",
        "import random\n",
        "import math\n",
        "import time\n",
        "import collections\n",
        "from typing import Sequence, Tuple, Text, Union, Optional, List\n",
        "\n",
        "import nvidia_smi\n",
        "\n",
        "import numpy as np\n",
        "import albumentations as A\n",
        "\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch.nn import functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "import sleap\n",
        "\n",
        "sleap.versions()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PWl3cG1_9WTK"
      },
      "source": [
        "# Utils"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "X2ESLmrn9VnH"
      },
      "outputs": [],
      "source": [
        "def seed_everything(seed=42):\n",
        "    random.seed(seed)\n",
        "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed(seed)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "\n",
        "def get_vram():\n",
        "    nvidia_smi.nvmlInit()\n",
        "\n",
        "    deviceCount = nvidia_smi.nvmlDeviceGetCount()\n",
        "    for i in range(deviceCount):\n",
        "        handle = nvidia_smi.nvmlDeviceGetHandleByIndex(i)\n",
        "        info = nvidia_smi.nvmlDeviceGetMemoryInfo(handle)\n",
        "        output = (\"Device {}: {}, Memory : ({:.2f}% free): {} (total), {} (free), {} (used)\"\n",
        "              .format(i, nvidia_smi.nvmlDeviceGetName(handle), 100*info.free/info.total,\n",
        "                      info.total/(1024 ** 3), info.free/(1024 ** 3), info.used/(1024 ** 3)))\n",
        "\n",
        "    nvidia_smi.nvmlShutdown()\n",
        "\n",
        "    return output\n",
        "\n",
        "def get_param_count(model):\n",
        "  trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "  total_params = sum(p.numel() for p in model.parameters())\n",
        "  nontrainable_params = total_params - trainable_params\n",
        "\n",
        "  return trainable_params, nontrainable_params, total_params"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "2pEmfBalS51R"
      },
      "outputs": [],
      "source": [
        "seed = 42\n",
        "seed_everything(seed)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wSdTJYOdu4L6"
      },
      "source": [
        "# Download training data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sDIF3RKdM86u",
        "outputId": "87087e3d-3c7c-43d3-f6f2-14cf67843fca"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "100  619M  100  619M    0     0  28.7M      0  0:00:21  0:00:21 --:--:-- 30.8M\n",
            "total 620M\n",
            "drwxr-xr-x 1 root root 4.0K Jun 27 17:52 .\n",
            "drwxr-xr-x 1 root root 4.0K Jun 27 17:48 ..\n",
            "drwxr-xr-x 4 root root 4.0K Jun 26 13:34 .config\n",
            "-rw-r--r-- 1 root root 620M Jun 27 17:53 labels.slp\n",
            "drwxr-xr-x 1 root root 4.0K Jun 26 13:35 sample_data\n",
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "100 77.2M  100 77.2M    0     0  18.3M      0  0:00:04  0:00:04 --:--:-- 18.3M\n",
            "total 697M\n",
            "drwxr-xr-x 1 root root 4.0K Jun 27 17:53 .\n",
            "drwxr-xr-x 1 root root 4.0K Jun 27 17:48 ..\n",
            "drwxr-xr-x 4 root root 4.0K Jun 26 13:34 .config\n",
            "-rw-r--r-- 1 root root 620M Jun 27 17:53 labels.slp\n",
            "drwxr-xr-x 1 root root 4.0K Jun 26 13:35 sample_data\n",
            "-rw-r--r-- 1 root root  78M Jun 27 17:53 val_labels.slp\n"
          ]
        }
      ],
      "source": [
        "!curl -L --output labels.slp https://storage.googleapis.com/sleap-data/datasets/wt_gold.13pt/tracking_split2/train.pkg.slp\n",
        "!ls -lah\n",
        "\n",
        "!curl -L --output val_labels.slp https://storage.googleapis.com/sleap-data/datasets/wt_gold.13pt/tracking_split2/val.pkg.slp\n",
        "!ls -lah"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5lugn7a_HC0Q"
      },
      "source": [
        "# Load the training data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UMkSIZrTHCMr",
        "outputId": "46aea60d-36b1-4631-fbf6-569ee642f4b1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Skeleton: Skeleton(description=None, nodes=[head, thorax, abdomen, wingL, wingR, forelegL4, forelegR4, midlegL4, midlegR4, hindlegL4, hindlegR4, eyeL, eyeR], edges=[thorax->head, thorax->abdomen, thorax->wingL, thorax->wingR, thorax->forelegL4, thorax->forelegR4, thorax->midlegL4, thorax->midlegR4, thorax->hindlegL4, thorax->hindlegR4, head->eyeL, head->eyeR], symmetries=[hindlegL4<->hindlegR4, eyeL<->eyeR, wingL<->wingR, midlegL4<->midlegR4, forelegL4<->forelegR4])\n",
            "Videos: ['labels.slp', 'labels.slp', 'labels.slp', 'labels.slp', 'labels.slp', 'labels.slp', 'labels.slp', 'labels.slp', 'labels.slp', 'labels.slp', 'labels.slp', 'labels.slp', 'labels.slp', 'labels.slp', 'labels.slp', 'labels.slp', 'labels.slp', 'labels.slp', 'labels.slp', 'labels.slp', 'labels.slp', 'labels.slp', 'labels.slp', 'labels.slp', 'labels.slp', 'labels.slp', 'labels.slp', 'labels.slp', 'labels.slp', 'labels.slp']\n",
            "Frames (user/predicted): 1,600/0\n",
            "Instances (user/predicted): 3,200/0\n",
            "Tracks: [Track(spawned_on=0, name='female'), Track(spawned_on=0, name='male')]\n",
            "Suggestions: 0\n",
            "Provenance: {}\n"
          ]
        }
      ],
      "source": [
        "# SLEAP Labels files (.slp) can include the images as well as labeled instances and\n",
        "# other metadata for a project.\n",
        "labels = sleap.load_file(\"labels.slp\")\n",
        "labels = labels.with_user_labels_only()\n",
        "labels.describe()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sklyy8NWEyff",
        "outputId": "e7acf35c-d0e0-436a-dade-5e24d0594821"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Skeleton: Skeleton(description=None, nodes=[head, thorax, abdomen, wingL, wingR, forelegL4, forelegR4, midlegL4, midlegR4, hindlegL4, hindlegR4, eyeL, eyeR], edges=[thorax->head, thorax->abdomen, thorax->wingL, thorax->wingR, thorax->forelegL4, thorax->forelegR4, thorax->midlegL4, thorax->midlegR4, thorax->hindlegL4, thorax->hindlegR4, head->eyeL, head->eyeR], symmetries=[wingL<->wingR, eyeL<->eyeR, forelegL4<->forelegR4, midlegL4<->midlegR4, hindlegL4<->hindlegR4])\n",
            "Videos: ['val_labels.slp', 'val_labels.slp', 'val_labels.slp', 'val_labels.slp', 'val_labels.slp', 'val_labels.slp', 'val_labels.slp', 'val_labels.slp', 'val_labels.slp', 'val_labels.slp', 'val_labels.slp', 'val_labels.slp', 'val_labels.slp', 'val_labels.slp', 'val_labels.slp', 'val_labels.slp', 'val_labels.slp', 'val_labels.slp', 'val_labels.slp', 'val_labels.slp', 'val_labels.slp', 'val_labels.slp', 'val_labels.slp', 'val_labels.slp', 'val_labels.slp', 'val_labels.slp', 'val_labels.slp', 'val_labels.slp', 'val_labels.slp', 'val_labels.slp']\n",
            "Frames (user/predicted): 200/0\n",
            "Instances (user/predicted): 400/0\n",
            "Tracks: [Track(spawned_on=0, name='female'), Track(spawned_on=0, name='male')]\n",
            "Suggestions: 0\n",
            "Provenance: {}\n"
          ]
        }
      ],
      "source": [
        "# Let's also do the same for the val labels.\n",
        "val_labels = sleap.load_file(\"val_labels.slp\")\n",
        "val_labels = val_labels.with_user_labels_only()\n",
        "val_labels.describe()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EK8DDPHDHBr_",
        "outputId": "0cc0ac2c-3569-4de0-a057-164a608511e3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of labels: 1600\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "LabeledFrame(video=HDF5Video('labels.slp'), frame_idx=166050, instances=2)"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ],
      "source": [
        "# Labels are list-like containers whose elements are LabeledFrames\n",
        "print(f\"Number of labels: {len(labels)}\")\n",
        "\n",
        "labeled_frame = labels[0]\n",
        "labeled_frame"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pP5uXrU3HX6o",
        "outputId": "de06d577-e487-4570-b0d0-641f4da107fa"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Instance(video=Video(filename=labels.slp, shape=(66, 1024, 1024, 1), backend=HDF5Video), frame_idx=166050, points=[head: (491.6, 187.7), thorax: (474.4, 224.8), abdomen: (459.9, 262.2), wingL: (448.3, 271.7), wingR: (452.1, 273.5), forelegL4: (478.5, 175.9), forelegR4: (499.9, 177.9), midlegL4: (440.6, 216.4), midlegR4: (510.1, 242.7), hindlegL4: (437.2, 234.3), hindlegR4: (490.9, 266.7), eyeL: (477.5, 193.2), eyeR: (498.4, 201.2)], track=Track(spawned_on=0, name='female'))"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ],
      "source": [
        "# LabeledFrames are containers for instances that were labeled in a single frame\n",
        "instance = labeled_frame[0]\n",
        "instance"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "700fTPs4Hurg",
        "outputId": "d96fef30-b0a3-4dae-9dd6-bc02680c89ec"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "rec.array([[491.58118169, 187.72078779],\n",
              "           [474.3603939 , 224.80196948],\n",
              "           [459.90098474, 262.16236338],\n",
              "           [448.26137864, 271.72078779],\n",
              "           [452.08118169, 273.54059084],\n",
              "           [478.5       , 175.90098474],\n",
              "           [499.94157558, 177.90098474],\n",
              "           [440.58118169, 216.3603939 ],\n",
              "           [510.12177253, 242.72078779],\n",
              "           [         nan,          nan],\n",
              "           [490.90098474, 266.72078779],\n",
              "           [477.54059084, 193.16236338],\n",
              "           [498.40098474, 201.18019695]],\n",
              "          dtype=float64)"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ],
      "source": [
        "# They can be converted to numpy arrays where each row corresponds to the coordinates\n",
        "# of a different body part:\n",
        "pts = instance.numpy()\n",
        "pts"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-vYsPusvviiu"
      },
      "source": [
        "# Setup training data generation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "l8Q-1ehtVWbr"
      },
      "outputs": [],
      "source": [
        "def update_not_shown_nodes(not_shown_nodes, node_names, new_nodes):\n",
        "  nodes_not_in_aug = np.array(list(set(node_names).difference(set(new_nodes))))\n",
        "  not_shown_in_aug_or_original_ind = np.in1d(node_names, nodes_not_in_aug).nonzero()[0]\n",
        "  not_shown_nodes[not_shown_in_aug_or_original_ind] = True\n",
        "\n",
        "  return not_shown_nodes\n",
        "\n",
        "\n",
        "def update_kp(kp, not_shown_nodes, node_names, new_nodes):\n",
        "  shown_after_aug_ind = np.in1d(node_names, new_nodes).nonzero()[0]\n",
        "  shown_ind = np.in1d(not_shown_nodes, True).nonzero()[0]\n",
        "\n",
        "  assert len(shown_after_aug_ind) == kp.shape[0]\n",
        "\n",
        "  kp_ = np.zeros((len(node_names), 2))\n",
        "  kp_[shown_after_aug_ind] = kp\n",
        "  kp_[shown_ind] = 0\n",
        "\n",
        "  return kp_\n",
        "\n",
        "\n",
        "def make_grid_vectors(\n",
        "    image_height: int, image_width: int, output_stride: int = 1):\n",
        "\n",
        "    xv = torch.arange(0, image_width, step=output_stride).to(torch.float32)\n",
        "    yv = torch.arange(0, image_height, step=output_stride).to(torch.float32)\n",
        "    return xv, yv\n",
        "\n",
        "def make_confmaps(\n",
        "    points: torch.Tensor, xv: torch.Tensor, yv: torch.Tensor, sigma: float):\n",
        "\n",
        "    x = torch.reshape(points[:, 0], (1, 1, -1))\n",
        "    y = torch.reshape(points[:, 1], (1, 1, -1))\n",
        "    cm = torch.exp(\n",
        "        -((torch.reshape(xv, (1, -1, 1)) - x) ** 2 + (torch.reshape(yv, (-1, 1, 1)) - y) ** 2)\n",
        "        / (2 * sigma ** 2)\n",
        "    )\n",
        "\n",
        "    # Replace NaNs with 0.\n",
        "    cm = torch.where(torch.isnan(cm), 0.0, cm)\n",
        "    return cm\n",
        "\n",
        "def get_bbox_coords_on_centroid(anchor_coords, crop_size, img_size):\n",
        "    (cx, cy) = anchor_coords\n",
        "\n",
        "    # [bottom left     top right]\n",
        "    # [  x1, y1,         x2, y2 ]\n",
        "    bbox = [\n",
        "        max(-crop_size / 2 + cx, 0),\n",
        "        max(-crop_size / 2 + cy, 0),\n",
        "        min(crop_size / 2 + cx, img_size[0]),\n",
        "        min(crop_size / 2 + cy, img_size[1])\n",
        "    ]\n",
        "\n",
        "    return bbox\n",
        "\n",
        "# My refactored version of this dataset generator.\n",
        "class DataGenerator(Dataset):\n",
        "    def __init__(self,\n",
        "      labels,\n",
        "      img_size=160,\n",
        "      anchor_name=\"thorax\",\n",
        "      sigma=1.5,\n",
        "      output_stride=2,\n",
        "      rot_range=(-180, 180),\n",
        "      is_train=True\n",
        "    ):\n",
        "        self.labels = labels.with_user_labels_only()\n",
        "        self.labels.remove_empty_instances(keep_empty_frames=False)\n",
        "\n",
        "        self.indices = []\n",
        "        for frame_idx, l in enumerate(self.labels):\n",
        "          inst_indices = np.arange(0, len(l.instances)).tolist()\n",
        "          self.indices.extend([(frame_idx, i) for i in inst_indices])\n",
        "\n",
        "        self.img_size = img_size\n",
        "\n",
        "        assert anchor_name in self.labels.skeleton.node_names\n",
        "        self.anchor_name = anchor_name\n",
        "\n",
        "        # Assuming 1 skeleton.\n",
        "        assert len(labels.skeletons) == 1\n",
        "        self.node_names = labels.skeletons[0].node_names\n",
        "\n",
        "        self.sigma = sigma\n",
        "        self.output_stride = output_stride\n",
        "        self.rot_range = rot_range\n",
        "\n",
        "        self.tfm = A.Compose([\n",
        "            A.Rotate(limit=list(self.rot_range), p=0.5)\n",
        "        ], keypoint_params=A.KeypointParams(format='xy', label_fields=['class_labels']))\n",
        "\n",
        "        self.xv, self.yv = make_grid_vectors(\n",
        "            image_height=self.img_size,\n",
        "            image_width=self.img_size,\n",
        "            output_stride=self.output_stride\n",
        "        )\n",
        "\n",
        "        self.is_train = is_train\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.indices)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        frame_idx, inst_idx = self.indices[idx]\n",
        "        lf = self.labels[frame_idx]\n",
        "        instance = lf[inst_idx]\n",
        "        img = lf.image\n",
        "        kp = instance.numpy()\n",
        "\n",
        "        # NaNs to 0 and clip.\n",
        "        assert kp.shape == (len(self.node_names), 2)\n",
        "        not_shown_nodes = np.isnan(kp).any(axis=1)\n",
        "        kp = np.nan_to_num(kp, nan=0)\n",
        "        kp = np.concatenate((np.clip(kp[:, :1], 0, img.shape[1]),\n",
        "                              np.clip(kp[:, 1:], 0, img.shape[0])),\n",
        "                            axis=1)\n",
        "\n",
        "        if self.is_train:\n",
        "            # Apply augmentations.\n",
        "            output = self.tfm(image=img, keypoints=kp, class_labels=self.node_names)\n",
        "            img, kp, new_nodes = output[\"image\"], np.array(output[\"keypoints\"]), output[\"class_labels\"]\n",
        "\n",
        "            # Update not_shown_nodes and kp.\n",
        "            not_shown_nodes = update_not_shown_nodes(not_shown_nodes, self.node_names, new_nodes)\n",
        "            kp = update_kp(kp, not_shown_nodes, self.node_names, new_nodes)\n",
        "\n",
        "        # Get bbox coordinate based on centroid.\n",
        "        bbox = get_bbox_coords_on_centroid(\n",
        "          kp[self.node_names.index(self.anchor_name)].tolist(),\n",
        "          self.img_size, img.shape[:2]\n",
        "        )\n",
        "\n",
        "        # Crop and pad.\n",
        "        x1, y1, x2, y2 = bbox\n",
        "        tfm_crop = A.Compose([\n",
        "          A.Crop(int(round(x1)), int(round(y1)), int(round(x2)), int(round(y2))),\n",
        "          A.PadIfNeeded(min_height=self.img_size, min_width=self.img_size)\n",
        "        ], keypoint_params=A.KeypointParams(format='xy', label_fields=['class_labels']))\n",
        "\n",
        "        crop_kp = tfm_crop(image=img, keypoints=kp, class_labels=self.node_names)\n",
        "        crop, kp, new_nodes = crop_kp[\"image\"], np.array(crop_kp[\"keypoints\"]), crop_kp[\"class_labels\"]\n",
        "        crop = torch.Tensor(crop).permute(2, 0, 1)\n",
        "\n",
        "        # Update not_shown_nodes and kp.\n",
        "        not_shown_nodes = update_not_shown_nodes(not_shown_nodes, self.node_names, new_nodes)\n",
        "        kp = update_kp(kp, not_shown_nodes, self.node_names, new_nodes)\n",
        "        kp = torch.Tensor(kp)\n",
        "\n",
        "        # Get confidence map.\n",
        "        xv, yv = make_grid_vectors(\n",
        "          image_height=self.img_size,\n",
        "          image_width=self.img_size,\n",
        "          output_stride=self.output_stride\n",
        "        )\n",
        "\n",
        "        cm = make_confmaps(\n",
        "          points=kp,\n",
        "          xv=self.xv,\n",
        "          yv=self.yv,\n",
        "          sigma=self.sigma\n",
        "        )\n",
        "        cm = cm.permute(2, 0, 1)\n",
        "\n",
        "        return crop, kp, cm"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0yo95XzRKkba"
      },
      "source": [
        "# Setting up a neural network model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "fCMnTHR9VetA"
      },
      "outputs": [],
      "source": [
        "class MaxPool2dWithSamePadding(nn.MaxPool2d):\n",
        "\n",
        "    def _calc_same_pad(self, i: int, k: int, s: int, d: int) -> int:\n",
        "        return max((math.ceil(i / s) - 1) * s + (k - 1) * d + 1 - i, 0)\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        if self.padding == \"same\":\n",
        "            ih, iw = x.size()[-2:]\n",
        "\n",
        "            pad_h = self._calc_same_pad(i=ih,\n",
        "                                        k=self.kernel_size if type(self.kernel_size) is int else self.kernel_size[0],\n",
        "                                        s=self.stride if type(self.stride) is int else self.stride[0],\n",
        "                                        d=self.dilation if type(self.dilation) is int else self.dilation[0])\n",
        "            pad_w = self._calc_same_pad(i=iw,\n",
        "                                        k=self.kernel_size if type(self.kernel_size) is int else self.kernel_size[1],\n",
        "                                        s=self.stride if type(self.stride) is int else self.stride[1],\n",
        "                                        d=self.dilation if type(self.dilation) is int else self.dilation[1])\n",
        "\n",
        "            if pad_h > 0 or pad_w > 0:\n",
        "                x = F.pad(\n",
        "                    x, [pad_w // 2, pad_w - pad_w // 2, pad_h // 2, pad_h - pad_h // 2]\n",
        "                )\n",
        "            self.padding = 0\n",
        "\n",
        "        return F.max_pool2d(x, self.kernel_size, self.stride,\n",
        "                                    self.padding, self.dilation, ceil_mode=self.ceil_mode,\n",
        "                                    return_indices=self.return_indices)\n",
        "\n",
        "def get_act_fn(activation: str) -> nn.Module:\n",
        "    activations = {\n",
        "        'relu': nn.ReLU(),\n",
        "        'sigmoid': nn.Sigmoid(),\n",
        "        'tanh': nn.Tanh()\n",
        "    }\n",
        "\n",
        "    return activations[activation]\n",
        "\n",
        "class SimpleConvBlock(nn.Module):\n",
        "    def __init__(self,\n",
        "        in_channels: int,\n",
        "        pool: bool = True,\n",
        "        pooling_stride: int = 2,\n",
        "        pool_before_convs: bool = False,\n",
        "        num_convs: int = 2,\n",
        "        filters: int = 32,\n",
        "        kernel_size: int = 3,\n",
        "        use_bias: bool = True,\n",
        "        batch_norm: bool = False,\n",
        "        activation: Text = \"relu\"\n",
        "    ) -> None:\n",
        "        super().__init__()\n",
        "\n",
        "        self.in_channels = in_channels\n",
        "        self.pool = pool\n",
        "        self.pooling_stride = pooling_stride\n",
        "        self.pool_before_convs = pool_before_convs\n",
        "        self.num_convs = num_convs\n",
        "        self.filters = filters\n",
        "        self.kernel_size = kernel_size\n",
        "        self.use_bias = use_bias\n",
        "        self.batch_norm = batch_norm\n",
        "        self.activation = activation\n",
        "\n",
        "        self.blocks = []\n",
        "        if pool and pool_before_convs:\n",
        "            self.blocks.append(\n",
        "                MaxPool2dWithSamePadding(\n",
        "                    kernel_size=2,\n",
        "                    stride=pooling_stride,\n",
        "                    padding=\"same\"\n",
        "                )\n",
        "            )\n",
        "\n",
        "        for i in range(num_convs):\n",
        "            self.blocks.append(\n",
        "                nn.Conv2d(\n",
        "                    in_channels=in_channels if i == 0 else filters,\n",
        "                    out_channels=filters,\n",
        "                    kernel_size=kernel_size,\n",
        "                    stride=1,\n",
        "                    padding=\"same\",\n",
        "                    bias=use_bias\n",
        "                )\n",
        "            )\n",
        "\n",
        "            if batch_norm:\n",
        "                self.blocks.append(\n",
        "                    nn.BatchNorm2d(filters)\n",
        "                )\n",
        "\n",
        "            self.blocks.append(\n",
        "                get_act_fn(activation)\n",
        "            )\n",
        "\n",
        "\n",
        "        if pool and not pool_before_convs:\n",
        "            self.blocks.append(\n",
        "                MaxPool2dWithSamePadding(\n",
        "                    kernel_size=2,\n",
        "                    stride=pooling_stride,\n",
        "                    padding=\"same\"\n",
        "                )\n",
        "            )\n",
        "\n",
        "        self.blocks = nn.Sequential(*self.blocks)\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        return self.blocks(x)\n",
        "\n",
        "class Encoder(nn.Module):\n",
        "    def __init__(self,\n",
        "        in_channels: int = 3,\n",
        "        filters: int = 64,\n",
        "        down_blocks: int = 4,\n",
        "        filters_rate: Union[float, int] = 2,\n",
        "        current_stride: int = 2,\n",
        "        stem_blocks: int = 0,\n",
        "        convs_per_block: int = 2,\n",
        "        kernel_size: Union[int, Tuple[int, int]] = 3,\n",
        "        middle_block: bool = True,\n",
        "        block_contraction: bool = False\n",
        "    ) -> None:\n",
        "        super().__init__()\n",
        "\n",
        "        self.in_channels = in_channels\n",
        "        self.filters = filters\n",
        "        self.down_blocks = down_blocks\n",
        "        self.filters_rate = filters_rate\n",
        "        self.current_stride = current_stride\n",
        "        self.stem_blocks = stem_blocks\n",
        "        self.convs_per_block = convs_per_block\n",
        "        self.kernel_size = kernel_size\n",
        "        self.middle_block = middle_block\n",
        "        self.block_contraction = block_contraction\n",
        "\n",
        "        self.encoder_stack = nn.ModuleList([])\n",
        "        for block in range(down_blocks):\n",
        "            prev_block_filters = -1 if block==0 else block_filters\n",
        "            block_filters = int(\n",
        "                filters * (filters_rate ** (block + stem_blocks))\n",
        "            )\n",
        "\n",
        "            self.encoder_stack.append(\n",
        "                SimpleConvBlock(\n",
        "                    in_channels=in_channels if block == 0 else prev_block_filters,\n",
        "                    pool=(block > 0),\n",
        "                    pool_before_convs=True,\n",
        "                    pooling_stride=2,\n",
        "                    num_convs=convs_per_block,\n",
        "                    filters=block_filters,\n",
        "                    kernel_size=kernel_size,\n",
        "                    use_bias=True,\n",
        "                    batch_norm=False,\n",
        "                    activation=\"relu\"\n",
        "                )\n",
        "            )\n",
        "        after_block_filters = block_filters\n",
        "\n",
        "        self.encoder_stack.append(\n",
        "            MaxPool2dWithSamePadding(\n",
        "                kernel_size=2,\n",
        "                stride=2,\n",
        "                padding=\"same\"\n",
        "            )\n",
        "        )\n",
        "\n",
        "        # Create a middle block (like the CARE implementation).\n",
        "        if middle_block:\n",
        "            if convs_per_block > 1:\n",
        "                # First convs are one exponent higher than the last encoder block.\n",
        "                block_filters = int(\n",
        "                    filters * (filters_rate ** (down_blocks + stem_blocks))\n",
        "                )\n",
        "                self.encoder_stack.append(\n",
        "                    SimpleConvBlock(\n",
        "                        in_channels=after_block_filters,\n",
        "                        pool=False,\n",
        "                        pool_before_convs=False,\n",
        "                        pooling_stride=2,\n",
        "                        num_convs=convs_per_block - 1,\n",
        "                        filters=block_filters,\n",
        "                        kernel_size=kernel_size,\n",
        "                        use_bias=True,\n",
        "                        batch_norm=False,\n",
        "                        activation=\"relu\",\n",
        "                    )\n",
        "                )\n",
        "\n",
        "            if block_contraction:\n",
        "                # Contract the channels with an exponent lower than the last encoder block.\n",
        "                block_filters = int(\n",
        "                    filters * (filters_rate ** (down_blocks + stem_blocks - 1))\n",
        "                )\n",
        "            else:\n",
        "                # Keep the block output filters the same.\n",
        "                block_filters = int(\n",
        "                    filters * (filters_rate ** (down_blocks + stem_blocks))\n",
        "                )\n",
        "\n",
        "            self.encoder_stack.append(\n",
        "                SimpleConvBlock(\n",
        "                    in_channels=block_filters,\n",
        "                    pool=False,\n",
        "                    pool_before_convs=False,\n",
        "                    pooling_stride=2,\n",
        "                    num_convs=1,\n",
        "                    filters=block_filters,\n",
        "                    kernel_size=kernel_size,\n",
        "                    use_bias=True,\n",
        "                    batch_norm=False,\n",
        "                    activation=\"relu\",\n",
        "                )\n",
        "            )\n",
        "\n",
        "        self.intermediate_features = {}\n",
        "        for i, block in enumerate(self.encoder_stack):\n",
        "            if isinstance(block, SimpleConvBlock) and block.pool:\n",
        "                current_stride *= block.pooling_stride\n",
        "\n",
        "            if current_stride not in self.intermediate_features.values():\n",
        "                self.intermediate_features[i] = current_stride\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        features = []\n",
        "        for i in range(len(self.encoder_stack)):\n",
        "            x = self.encoder_stack[i](x)\n",
        "\n",
        "            if i in self.intermediate_features.keys():\n",
        "                features.append(x)\n",
        "\n",
        "        return x, features[1:][::-1]\n",
        "\n",
        "class SimpleUpsamplingBlock(nn.Module):\n",
        "    def __init__(self,\n",
        "        x_in_shape: int,\n",
        "        current_stride: int,\n",
        "        upsampling_stride: int = 2,\n",
        "        interp_method: Text = \"bilinear\",\n",
        "        refine_convs: int = 2,\n",
        "        refine_convs_filters: int = 64,\n",
        "        refine_convs_kernel_size: int = 3,\n",
        "        refine_convs_use_bias: bool = True,\n",
        "        refine_convs_batch_norm: bool = True,\n",
        "        refine_convs_batch_norm_before_activation: bool = True,\n",
        "        refine_convs_activation: Text = \"relu\"\n",
        "    ) -> None:\n",
        "        super().__init__()\n",
        "\n",
        "        self.x_in_shape = x_in_shape\n",
        "        self.current_stride = current_stride\n",
        "        self.upsampling_stride = upsampling_stride\n",
        "        self.interp_method = interp_method\n",
        "        self.refine_convs = refine_convs\n",
        "        self.refine_convs_filters = refine_convs_filters\n",
        "        self.refine_convs_kernel_size = refine_convs_kernel_size\n",
        "        self.refine_convs_use_bias = refine_convs_use_bias\n",
        "        self.refine_convs_batch_norm = refine_convs_batch_norm\n",
        "        self.refine_convs_batch_norm_before_activation = refine_convs_batch_norm_before_activation\n",
        "        self.refine_convs_activation = refine_convs_activation\n",
        "\n",
        "        self.blocks = nn.ModuleList([])\n",
        "        if current_stride is not None:\n",
        "            # Append the strides to the block prefix.\n",
        "            new_stride = current_stride // upsampling_stride\n",
        "\n",
        "        # Upsample via interpolation.\n",
        "        self.blocks.append(\n",
        "            nn.Upsample(\n",
        "                scale_factor=upsampling_stride,\n",
        "                mode=interp_method,\n",
        "            )\n",
        "        )\n",
        "\n",
        "\n",
        "        # Add further convolutions to refine after upsampling and/or skip.\n",
        "        for i in range(refine_convs):\n",
        "            filters = refine_convs_filters\n",
        "            self.blocks.append(\n",
        "                nn.Conv2d(\n",
        "                    in_channels=x_in_shape if i==0 else filters,\n",
        "                    out_channels=filters,\n",
        "                    kernel_size=refine_convs_kernel_size,\n",
        "                    stride=1,\n",
        "                    padding=\"same\",\n",
        "                    bias=refine_convs_use_bias\n",
        "                )\n",
        "            )\n",
        "\n",
        "            if (\n",
        "                refine_convs_batch_norm\n",
        "                and refine_convs_batch_norm_before_activation\n",
        "            ):\n",
        "                self.blocks.append(nn.BatchNorm2d(num_features=filters))\n",
        "\n",
        "\n",
        "            self.blocks.append(\n",
        "                get_act_fn(refine_convs_activation)\n",
        "            )\n",
        "\n",
        "            if (\n",
        "                refine_convs_batch_norm\n",
        "                and not refine_convs_batch_norm_before_activation\n",
        "            ):\n",
        "                self.blocks.append(nn.BatchNorm2d(num_features=filters))\n",
        "\n",
        "\n",
        "    def forward(self, x: torch.Tensor, feature: torch.Tensor) -> torch.Tensor:\n",
        "        for idx, b in enumerate(self.blocks):\n",
        "            if idx == 1:  # Right after upsampling or convtranspose2d.\n",
        "                x = torch.concat((x, feature), dim=1)\n",
        "            x = b(x)\n",
        "\n",
        "        return x\n",
        "\n",
        "class Decoder(nn.Module):\n",
        "    def __init__(self,\n",
        "        x_in_shape: int,\n",
        "        current_stride: int,\n",
        "        filters: int = 64,\n",
        "        up_blocks: int = 4,\n",
        "        down_blocks: int = 3,\n",
        "        filters_rate: int = 2,\n",
        "        stem_blocks: int = 0,\n",
        "        convs_per_block: int = 2,\n",
        "        kernel_size: int = 3,\n",
        "        block_contraction: bool = False\n",
        "    ) -> None:\n",
        "        super().__init__()\n",
        "\n",
        "        self.x_in_shape = x_in_shape\n",
        "        self.current_stride = current_stride\n",
        "        self.filters = filters\n",
        "        self.up_blocks = up_blocks\n",
        "        self.down_blocks = down_blocks\n",
        "        self.filters_rate = filters_rate\n",
        "        self.stem_blocks = stem_blocks\n",
        "        self.convs_per_block = convs_per_block\n",
        "        self.kernel_size = kernel_size\n",
        "        self.block_contraction = block_contraction\n",
        "\n",
        "        self.decoder_stack = nn.ModuleList([])\n",
        "        for block in range(up_blocks):\n",
        "            prev_block_filters_in = -1 if block == 0 else block_filters_in\n",
        "            block_filters_in = int(\n",
        "                filters\n",
        "                * (\n",
        "                    filters_rate\n",
        "                    ** (down_blocks + stem_blocks - 1 - block)\n",
        "                )\n",
        "            )\n",
        "            if block_contraction:\n",
        "                block_filters_out = int(\n",
        "                    filters\n",
        "                    * (\n",
        "                        filters_rate\n",
        "                        ** (down_blocks + stem_blocks - 2 - block)\n",
        "                    )\n",
        "                )\n",
        "            else:\n",
        "                block_filters_out = block_filters_in\n",
        "\n",
        "            next_stride = current_stride // 2\n",
        "\n",
        "            self.decoder_stack.append(\n",
        "                SimpleUpsamplingBlock(\n",
        "                    x_in_shape=(x_in_shape + block_filters_in) if block == 0 else (prev_block_filters_in + block_filters_in),\n",
        "                    current_stride=current_stride,\n",
        "                    upsampling_stride=2,\n",
        "                    interp_method=\"bilinear\",\n",
        "                    refine_convs=self.convs_per_block,\n",
        "                    refine_convs_filters=block_filters_out,\n",
        "                    refine_convs_kernel_size=self.kernel_size,\n",
        "                    refine_convs_batch_norm=False,\n",
        "                )\n",
        "            )\n",
        "\n",
        "            current_stride = next_stride\n",
        "\n",
        "    def forward(self, x: torch.Tensor, features: List[torch.Tensor]) -> torch.Tensor:\n",
        "        for i in range(len(self.decoder_stack)):\n",
        "            x = self.decoder_stack[i](x, features[i])\n",
        "\n",
        "        return x\n",
        "\n",
        "class Unet(nn.Module):\n",
        "    def __init__(self,\n",
        "        in_channels: int = 1,\n",
        "        kernel_size: int = 3,\n",
        "        filters: int = 32,\n",
        "        filters_rate: int = 1.5,\n",
        "        stem_blocks: int = 0,\n",
        "        down_blocks: int = 4,\n",
        "        up_blocks: int = 3,\n",
        "        convs_per_block: int = 2,\n",
        "        middle_block: bool = True,\n",
        "        block_contraction: bool = False\n",
        "    ) -> None:\n",
        "        super().__init__()\n",
        "\n",
        "        self.enc = Encoder(\n",
        "            in_channels=in_channels,\n",
        "            filters=filters,\n",
        "            down_blocks=down_blocks,\n",
        "            filters_rate=filters_rate,\n",
        "            stem_blocks=stem_blocks,\n",
        "            convs_per_block=convs_per_block,\n",
        "            kernel_size=kernel_size,\n",
        "            middle_block=middle_block,\n",
        "            block_contraction=block_contraction\n",
        "        )\n",
        "\n",
        "        current_stride = int(\n",
        "            np.prod(\n",
        "                [block.pooling_stride for block in self.enc.encoder_stack if hasattr(block, \"pool\") and block.pool]\n",
        "                + [1]\n",
        "            )\n",
        "        )\n",
        "\n",
        "        x_in_shape = int(\n",
        "            filters * (filters_rate ** (down_blocks + stem_blocks))\n",
        "        )\n",
        "\n",
        "        self.dec = Decoder(x_in_shape=x_in_shape, current_stride=current_stride, filters=filters,\n",
        "            up_blocks=up_blocks, down_blocks=down_blocks, filters_rate=filters_rate\n",
        "        )\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        x, features = self.enc(x)\n",
        "        x = self.dec(x, features)\n",
        "        return x\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gEfgZT4hLyW4"
      },
      "source": [
        "# Train the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WFNfmlnprhmb",
        "outputId": "1b1b5c02-c093-4707-89a9-0e2fb3d8716c"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ],
      "source": [
        "import multiprocessing\n",
        "\n",
        "cores = multiprocessing.cpu_count()\n",
        "cores"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "FP7r_rzYvDy6"
      },
      "outputs": [],
      "source": [
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tDMSgbrrY7E0",
        "outputId": "f40137b3-ea54-4236-b6ca-be5af7951229"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 0 | Loss: 0.00715\n",
            "Epoch: 0 | Loss: 0.00030\n",
            "Epoch: 0 | Loss: 0.00028\n",
            "Epoch: 0 | Loss: 0.00027\n",
            "Epoch: 0 | Loss: 0.00026\n",
            "Epoch: 0 | Loss: 0.00026\n",
            "Epoch: 0 | Loss: 0.00027\n",
            "Epoch: 0 | Loss: 0.00027\n",
            "TRAIN: --- 80.82558941841125s seconds ---\n",
            "VAL: --- 5.74263334274292s seconds ---\n",
            "Epoch: 1 | Loss: 0.00026\n",
            "Epoch: 1 | Loss: 0.00026\n",
            "Epoch: 1 | Loss: 0.00025\n",
            "Epoch: 1 | Loss: 0.00024\n",
            "Epoch: 1 | Loss: 0.00024\n",
            "Epoch: 1 | Loss: 0.00022\n",
            "Epoch: 1 | Loss: 0.00022\n",
            "Epoch: 1 | Loss: 0.00022\n",
            "TRAIN: --- 84.83880734443665s seconds ---\n",
            "VAL: --- 7.007384300231934s seconds ---\n",
            "Epoch: 2 | Loss: 0.00022\n",
            "Epoch: 2 | Loss: 0.00021\n",
            "Epoch: 2 | Loss: 0.00020\n",
            "Epoch: 2 | Loss: 0.00021\n",
            "Epoch: 2 | Loss: 0.00018\n",
            "Epoch: 2 | Loss: 0.00022\n",
            "Epoch: 2 | Loss: 0.00018\n",
            "Epoch: 2 | Loss: 0.00017\n",
            "TRAIN: --- 80.88602590560913s seconds ---\n",
            "VAL: --- 8.731147527694702s seconds ---\n",
            "Epoch: 3 | Loss: 0.00019\n",
            "Epoch: 3 | Loss: 0.00017\n",
            "Epoch: 3 | Loss: 0.00018\n",
            "Epoch: 3 | Loss: 0.00016\n",
            "Epoch: 3 | Loss: 0.00018\n",
            "Epoch: 3 | Loss: 0.00016\n",
            "Epoch: 3 | Loss: 0.00018\n",
            "Epoch: 3 | Loss: 0.00017\n",
            "TRAIN: --- 85.04641819000244s seconds ---\n",
            "VAL: --- 5.703203916549683s seconds ---\n",
            "Epoch: 4 | Loss: 0.00019\n",
            "Epoch: 4 | Loss: 0.00017\n",
            "Epoch: 4 | Loss: 0.00018\n",
            "Epoch: 4 | Loss: 0.00020\n",
            "Epoch: 4 | Loss: 0.00014\n",
            "Epoch: 4 | Loss: 0.00017\n",
            "Epoch: 4 | Loss: 0.00016\n",
            "Epoch: 4 | Loss: 0.00016\n",
            "TRAIN: --- 80.91871428489685s seconds ---\n",
            "VAL: --- 8.509625911712646s seconds ---\n",
            "Epoch: 5 | Loss: 0.00014\n",
            "Epoch: 5 | Loss: 0.00013\n",
            "Epoch: 5 | Loss: 0.00016\n",
            "Epoch: 5 | Loss: 0.00015\n",
            "Epoch: 5 | Loss: 0.00013\n",
            "Epoch: 5 | Loss: 0.00012\n",
            "Epoch: 5 | Loss: 0.00014\n",
            "Epoch: 5 | Loss: 0.00014\n",
            "TRAIN: --- 85.23373675346375s seconds ---\n",
            "VAL: --- 6.871620416641235s seconds ---\n",
            "Epoch: 6 | Loss: 0.00011\n",
            "Epoch: 6 | Loss: 0.00016\n",
            "Epoch: 6 | Loss: 0.00013\n",
            "Epoch: 6 | Loss: 0.00016\n",
            "Epoch: 6 | Loss: 0.00017\n",
            "Epoch: 6 | Loss: 0.00012\n",
            "Epoch: 6 | Loss: 0.00010\n",
            "Epoch: 6 | Loss: 0.00009\n",
            "TRAIN: --- 80.8298134803772s seconds ---\n",
            "VAL: --- 5.760534286499023s seconds ---\n",
            "Epoch: 7 | Loss: 0.00013\n",
            "Epoch: 7 | Loss: 0.00010\n",
            "Epoch: 7 | Loss: 0.00008\n",
            "Epoch: 7 | Loss: 0.00009\n",
            "Epoch: 7 | Loss: 0.00013\n",
            "Epoch: 7 | Loss: 0.00010\n",
            "Epoch: 7 | Loss: 0.00009\n",
            "Epoch: 7 | Loss: 0.00010\n",
            "TRAIN: --- 84.46455836296082s seconds ---\n",
            "VAL: --- 9.883276224136353s seconds ---\n",
            "Epoch: 8 | Loss: 0.00007\n",
            "Epoch: 8 | Loss: 0.00013\n",
            "Epoch: 8 | Loss: 0.00010\n",
            "Epoch: 8 | Loss: 0.00010\n",
            "Epoch: 8 | Loss: 0.00009\n",
            "Epoch: 8 | Loss: 0.00010\n",
            "Epoch: 8 | Loss: 0.00017\n",
            "Epoch: 8 | Loss: 0.00007\n",
            "TRAIN: --- 81.66969752311707s seconds ---\n",
            "VAL: --- 5.978207349777222s seconds ---\n",
            "Epoch: 9 | Loss: 0.00009\n",
            "Epoch: 9 | Loss: 0.00015\n",
            "Epoch: 9 | Loss: 0.00008\n",
            "Epoch: 9 | Loss: 0.00008\n",
            "Epoch: 9 | Loss: 0.00010\n",
            "Epoch: 9 | Loss: 0.00009\n",
            "Epoch: 9 | Loss: 0.00011\n",
            "Epoch: 9 | Loss: 0.00007\n",
            "TRAIN: --- 84.8375313282013s seconds ---\n",
            "VAL: --- 6.164664268493652s seconds ---\n",
            "Epoch: 10 | Loss: 0.00008\n",
            "Epoch: 10 | Loss: 0.00009\n",
            "Epoch: 10 | Loss: 0.00011\n",
            "Epoch: 10 | Loss: 0.00006\n",
            "Epoch: 10 | Loss: 0.00013\n",
            "Epoch: 10 | Loss: 0.00007\n",
            "Epoch: 10 | Loss: 0.00009\n",
            "Epoch: 10 | Loss: 0.00010\n",
            "TRAIN: --- 80.57148909568787s seconds ---\n",
            "VAL: --- 9.538167715072632s seconds ---\n",
            "Epoch: 11 | Loss: 0.00008\n",
            "Epoch: 11 | Loss: 0.00008\n",
            "Epoch: 11 | Loss: 0.00010\n",
            "Epoch: 11 | Loss: 0.00008\n",
            "Epoch: 11 | Loss: 0.00012\n",
            "Epoch: 11 | Loss: 0.00008\n",
            "Epoch: 11 | Loss: 0.00009\n",
            "Epoch: 11 | Loss: 0.00008\n",
            "TRAIN: --- 85.2735652923584s seconds ---\n",
            "VAL: --- 5.714203596115112s seconds ---\n",
            "Epoch: 12 | Loss: 0.00007\n",
            "Epoch: 12 | Loss: 0.00006\n",
            "Epoch: 12 | Loss: 0.00007\n",
            "Epoch: 12 | Loss: 0.00010\n",
            "Epoch: 12 | Loss: 0.00013\n",
            "Epoch: 12 | Loss: 0.00011\n",
            "Epoch: 12 | Loss: 0.00008\n",
            "Epoch: 12 | Loss: 0.00007\n",
            "TRAIN: --- 81.05310893058777s seconds ---\n",
            "VAL: --- 10.031612157821655s seconds ---\n",
            "Epoch: 13 | Loss: 0.00008\n",
            "Epoch: 13 | Loss: 0.00006\n",
            "Epoch: 13 | Loss: 0.00009\n",
            "Epoch: 13 | Loss: 0.00007\n",
            "Epoch: 13 | Loss: 0.00011\n",
            "Epoch: 13 | Loss: 0.00013\n",
            "Epoch: 13 | Loss: 0.00006\n",
            "Epoch: 13 | Loss: 0.00009\n",
            "TRAIN: --- 84.69884991645813s seconds ---\n",
            "VAL: --- 5.728881120681763s seconds ---\n",
            "Epoch: 14 | Loss: 0.00007\n",
            "Epoch: 14 | Loss: 0.00009\n",
            "Epoch: 14 | Loss: 0.00011\n",
            "Epoch: 14 | Loss: 0.00008\n",
            "Epoch: 14 | Loss: 0.00005\n",
            "Epoch: 14 | Loss: 0.00006\n",
            "Epoch: 14 | Loss: 0.00011\n",
            "Epoch: 14 | Loss: 0.00013\n",
            "TRAIN: --- 81.04774451255798s seconds ---\n",
            "VAL: --- 6.807452440261841s seconds ---\n",
            "Epoch: 15 | Loss: 0.00010\n",
            "Epoch: 15 | Loss: 0.00010\n",
            "Epoch: 15 | Loss: 0.00007\n",
            "Epoch: 15 | Loss: 0.00006\n",
            "Epoch: 15 | Loss: 0.00011\n",
            "Epoch: 15 | Loss: 0.00007\n",
            "Epoch: 15 | Loss: 0.00010\n",
            "Epoch: 15 | Loss: 0.00009\n",
            "TRAIN: --- 83.4905014038086s seconds ---\n",
            "VAL: --- 10.086686134338379s seconds ---\n",
            "Epoch: 16 | Loss: 0.00014\n",
            "Epoch: 16 | Loss: 0.00009\n",
            "Epoch: 16 | Loss: 0.00014\n",
            "Epoch: 16 | Loss: 0.00005\n",
            "Epoch: 16 | Loss: 0.00005\n",
            "Epoch: 16 | Loss: 0.00007\n",
            "Epoch: 16 | Loss: 0.00006\n",
            "Epoch: 16 | Loss: 0.00009\n",
            "TRAIN: --- 80.93115878105164s seconds ---\n",
            "VAL: --- 5.728479623794556s seconds ---\n",
            "Epoch: 17 | Loss: 0.00008\n",
            "Epoch: 17 | Loss: 0.00007\n",
            "Epoch: 17 | Loss: 0.00006\n",
            "Epoch: 17 | Loss: 0.00006\n",
            "Epoch: 17 | Loss: 0.00007\n",
            "Epoch: 17 | Loss: 0.00010\n",
            "Epoch: 17 | Loss: 0.00006\n",
            "Epoch: 17 | Loss: 0.00011\n",
            "TRAIN: --- 81.13518929481506s seconds ---\n",
            "VAL: --- 7.432884216308594s seconds ---\n",
            "Epoch: 18 | Loss: 0.00008\n",
            "Epoch: 18 | Loss: 0.00010\n",
            "Epoch: 18 | Loss: 0.00009\n",
            "Epoch: 18 | Loss: 0.00009\n",
            "Epoch: 18 | Loss: 0.00009\n",
            "Epoch: 18 | Loss: 0.00012\n",
            "Epoch: 18 | Loss: 0.00008\n",
            "Epoch: 18 | Loss: 0.00005\n",
            "TRAIN: --- 82.5347306728363s seconds ---\n",
            "VAL: --- 8.618462562561035s seconds ---\n",
            "Epoch: 19 | Loss: 0.00006\n",
            "Epoch: 19 | Loss: 0.00008\n",
            "Epoch: 19 | Loss: 0.00008\n",
            "Epoch: 19 | Loss: 0.00007\n",
            "Epoch: 19 | Loss: 0.00010\n",
            "Epoch: 19 | Loss: 0.00008\n",
            "Epoch: 19 | Loss: 0.00008\n",
            "Epoch: 19 | Loss: 0.00006\n",
            "TRAIN: --- 80.65238618850708s seconds ---\n",
            "VAL: --- 7.382519721984863s seconds ---\n"
          ]
        }
      ],
      "source": [
        "filters = 32\n",
        "filters_rate = 1.5\n",
        "down_blocks = 4\n",
        "stem_blocks = 0\n",
        "up_blocks = 3\n",
        "\n",
        "train_ds = DataGenerator(labels)\n",
        "train_dl = DataLoader(\n",
        "    train_ds,\n",
        "    batch_size=4,\n",
        "    shuffle=True,\n",
        "    num_workers=cores,\n",
        "    pin_memory=True,\n",
        "    drop_last=True,\n",
        "    prefetch_factor=2\n",
        ")\n",
        "\n",
        "val_ds = DataGenerator(val_labels, is_train=False)\n",
        "val_dl = DataLoader(\n",
        "    val_ds,\n",
        "    batch_size=4,\n",
        "    shuffle=False,\n",
        "    num_workers=cores,\n",
        "    pin_memory=True,\n",
        "    drop_last=True,\n",
        "    prefetch_factor=2\n",
        ")\n",
        "\n",
        "unet = Unet(filters=filters,\n",
        "            filters_rate=filters_rate,\n",
        "            down_blocks=down_blocks,\n",
        "            stem_blocks=stem_blocks,\n",
        "            up_blocks=up_blocks)\n",
        "\n",
        "in_channels = int(\n",
        "    filters\n",
        "    * (\n",
        "        filters_rate\n",
        "        ** (down_blocks + stem_blocks - 1 - up_blocks + 1)\n",
        "    )\n",
        ")\n",
        "model = nn.Sequential(*[\n",
        "    unet,\n",
        "    nn.Conv2d(in_channels=in_channels, out_channels=13, kernel_size=1, padding=\"same\")\n",
        "])\n",
        "\n",
        "opt = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
        "scheduler = torch.optim.lr_scheduler.OneCycleLR(opt, max_lr=0.01, steps_per_epoch=len(train_dl), epochs=epochs)\n",
        "\n",
        "scaler = torch.cuda.amp.GradScaler()\n",
        "model = model.to(device)\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    _ = model.train()\n",
        "    start_time = time.time()\n",
        "    train_loss = 0\n",
        "    for idx, batch in enumerate(train_dl):\n",
        "        X, _, y = batch\n",
        "        X = X.to(device)\n",
        "        y = y.to(device)\n",
        "\n",
        "        with torch.autocast(device):\n",
        "            y_preds = model(X)\n",
        "            loss = nn.MSELoss()(y_preds, y)\n",
        "        scaler.scale(loss).backward()\n",
        "        scaler.step(opt)\n",
        "        scaler.update()\n",
        "        opt.zero_grad()\n",
        "\n",
        "        if idx % 100 == 0:\n",
        "          print(f\"Epoch: {epoch} | Loss: {loss:.5f}\")\n",
        "\n",
        "        train_loss += loss\n",
        "\n",
        "    train_loss /= (idx+1)\n",
        "    train_time = time.time() - start_time\n",
        "    print(f\"TRAIN: --- {train_time}s seconds ---\")\n",
        "\n",
        "    _  = model.eval()\n",
        "    start_time = time.time()\n",
        "    val_loss = 0\n",
        "    for idx, batch in enumerate(val_dl):\n",
        "        X, _, y = batch\n",
        "        X = X.to(\"cuda\")\n",
        "        y = y.to(\"cuda\")\n",
        "\n",
        "        with torch.no_grad():\n",
        "            y_preds = model(X)\n",
        "            loss = nn.MSELoss()(y_preds, y)\n",
        "\n",
        "        val_loss += loss\n",
        "\n",
        "    val_loss /= (idx+1)\n",
        "    val_time = time.time() - start_time\n",
        "    print(f\"VAL: --- {val_time}s seconds ---\")\n",
        "\n",
        "    scheduler.step()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TfCrTPwPOm_d"
      },
      "source": [
        "# Inference & Evaluation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dyb-Lsqs0gzI"
      },
      "source": [
        "## Utility Functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "lvEsjPCLOni8"
      },
      "outputs": [],
      "source": [
        "# 1. find instance peaks\n",
        "def find_peaks(cms, xv, yv):\n",
        "    \"\"\"Find peaks in a set of confidence maps via integral regression.\n",
        "\n",
        "    Args:\n",
        "        cms: A batch of confidence maps of shape (batch_size, height, width, n_points).\n",
        "        xv: X-sampling vector of shape (grid_width,).\n",
        "        yv: Y-sampling vector of shape (grid_width,).\n",
        "\n",
        "    Returns:\n",
        "        A set of estimated peaks of shape (batch_size, n_points, 2).\n",
        "\n",
        "    Notes:\n",
        "        This function can also accept confidence maps of shape (height, width, n_points)\n",
        "        and returns peaks as (n_points, 2).\n",
        "    \"\"\"\n",
        "    is_singleton = cms.ndim == 3\n",
        "    if is_singleton:\n",
        "        cms = np.expand_dims(cms, axis=0)\n",
        "\n",
        "    # Find integral over height and width.\n",
        "    z = cms.reshape(cms.shape[0], -1, cms.shape[-1]).sum(axis=1)\n",
        "\n",
        "    # Compute x- and y-coordinates.\n",
        "    x = (cms * xv.reshape(1, 1, -1, 1)).reshape(cms.shape[0], -1, cms.shape[-1]).sum(axis=1) / z\n",
        "    y = (cms * yv.reshape(1, -1, 1, 1)).reshape(cms.shape[0], -1, cms.shape[-1]).sum(axis=1) / z\n",
        "\n",
        "    # Stack the coordinates into (batch_size, n_points, 2).\n",
        "    pts_pr = np.stack([x, y], axis=-1)\n",
        "\n",
        "    if is_singleton:\n",
        "        pts_pr = pts_pr.squeeze(axis=0)\n",
        "    return pts_pr"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "CSHr435pcvMS"
      },
      "outputs": [],
      "source": [
        "def compute_instance_area(points: np.ndarray) -> np.ndarray:\n",
        "    \"\"\"Compute the area of the bounding box of a set of keypoints.\n",
        "\n",
        "    Args:\n",
        "        points: A numpy array of coordinates.\n",
        "\n",
        "    Returns:\n",
        "        The area of the bounding box of the points.\n",
        "    \"\"\"\n",
        "    if points.ndim == 2:\n",
        "        points = np.expand_dims(points, axis=0)\n",
        "\n",
        "    min_pt = np.nanmin(points, axis=-2)\n",
        "    max_pt = np.nanmax(points, axis=-2)\n",
        "\n",
        "    return np.prod(max_pt - min_pt, axis=-1)\n",
        "\n",
        "\n",
        "\n",
        "def compute_oks(\n",
        "    points_gt: np.ndarray,\n",
        "    points_pr: np.ndarray,\n",
        "    scale: Optional[float] = None,\n",
        "    stddev: float = 0.025,\n",
        ") -> np.ndarray:\n",
        "    \"\"\"Compute the object keypoints similarity between sets of points.\n",
        "\n",
        "    Args:\n",
        "        points_gt: Ground truth instances of shape (n_gt, n_nodes, n_ed),\n",
        "            where n_nodes is the number of body parts/keypoint types, and n_ed\n",
        "            is the number of Euclidean dimensions (typically 2 or 3). Keypoints\n",
        "            that are missing/not visible should be represented as NaNs.\n",
        "        points_pr: Predicted instance of shape (n_pr, n_nodes, n_ed).\n",
        "        scale: Size scaling factor to use when weighing the scores, typically\n",
        "            the area of the bounding box of the instance (in pixels). This\n",
        "            should be of the length n_gt. If a scalar is provided, the same\n",
        "            number is used for all ground truth instances. If set to None, the\n",
        "            bounding box area of the ground truth instances will be calculated.\n",
        "        stddev: The standard deviation associated with the spread in the\n",
        "            localization accuracy of each node/keypoint type. This should be of\n",
        "            the length n_nodes. \"Easier\" keypoint types will have lower values\n",
        "            to reflect the smaller spread expected in localizing it.\n",
        "\n",
        "    Returns:\n",
        "        The object keypoints similarity between every pair of ground truth and\n",
        "        predicted instance, a numpy array of of shape (n_gt, n_pr) in the range\n",
        "        of [0, 1.0], with 1.0 denoting a perfect match.\n",
        "\n",
        "    Notes:\n",
        "        It's important to set the stddev appropriately when accounting for the\n",
        "        difficulty of each keypoint type. For reference, the median value for\n",
        "        all keypoint types in COCO is 0.072. The \"easiest\" keypoint is the left\n",
        "        eye, with stddev of 0.025, since it is easy to precisely locate the\n",
        "        eyes when labeling. The \"hardest\" keypoint is the left hip, with stddev\n",
        "        of 0.107, since it's hard to locate the left hip bone without external\n",
        "        anatomical features and since it is often occluded by clothing.\n",
        "\n",
        "        The implementation here is based off of the descriptions in:\n",
        "        Ronch & Perona. \"Benchmarking and Error Diagnosis in Multi-Instance Pose\n",
        "        Estimation.\" ICCV (2017).\n",
        "    \"\"\"\n",
        "    if points_gt.ndim == 2:\n",
        "        points_gt = np.expand_dims(points_gt, axis=0)\n",
        "    if points_pr.ndim == 2:\n",
        "        points_pr = np.expand_dims(points_pr, axis=0)\n",
        "\n",
        "    if scale is None:\n",
        "        scale = compute_instance_area(points_gt)\n",
        "\n",
        "    n_gt, n_nodes, n_ed = points_gt.shape  # n_ed = 2 or 3 (euclidean dimensions)\n",
        "    n_pr = points_pr.shape[0]\n",
        "\n",
        "    # If scalar scale was provided, use the same for each ground truth instance.\n",
        "    if np.isscalar(scale):\n",
        "        scale = np.full(n_gt, scale)\n",
        "\n",
        "    # If scalar standard deviation was provided, use the same for each node.\n",
        "    if np.isscalar(stddev):\n",
        "        stddev = np.full(n_nodes, stddev)\n",
        "\n",
        "    # Compute displacement between each pair.\n",
        "    displacement = np.reshape(points_gt, (n_gt, 1, n_nodes, n_ed)) - np.reshape(\n",
        "        points_pr, (1, n_pr, n_nodes, n_ed)\n",
        "    )\n",
        "    assert displacement.shape == (n_gt, n_pr, n_nodes, n_ed)\n",
        "\n",
        "    # Convert to pairwise Euclidean distances.\n",
        "    distance = (displacement ** 2).sum(axis=-1)  # (n_gt, n_pr, n_nodes)\n",
        "    assert distance.shape == (n_gt, n_pr, n_nodes)\n",
        "\n",
        "    # Compute the normalization factor per keypoint.\n",
        "    spread_factor = (2 * stddev) ** 2\n",
        "    scale_factor = 2 * (scale + np.spacing(1))\n",
        "    normalization_factor = np.reshape(spread_factor, (1, 1, n_nodes)) * np.reshape(\n",
        "        scale_factor, (n_gt, 1, 1)\n",
        "    )\n",
        "    assert normalization_factor.shape == (n_gt, 1, n_nodes)\n",
        "\n",
        "    # Since a \"miss\" is considered as KS < 0.5, we'll set the\n",
        "    # distances for predicted points that are missing to inf.\n",
        "    missing_pr = np.any(np.isnan(points_pr), axis=-1)  # (n_pr, n_nodes)\n",
        "    assert missing_pr.shape == (n_pr, n_nodes)\n",
        "    distance[:, missing_pr] = np.inf\n",
        "\n",
        "    # Compute the keypoint similarity as per the top of Eq. 1.\n",
        "    ks = np.exp(-(distance / normalization_factor))  # (n_gt, n_pr, n_nodes)\n",
        "    assert ks.shape == (n_gt, n_pr, n_nodes)\n",
        "\n",
        "    # Set the KS for missing ground truth points to 0.\n",
        "    # This is equivalent to the visibility delta function of the bottom\n",
        "    # of Eq. 1.\n",
        "    missing_gt = np.any(np.isnan(points_gt), axis=-1)  # (n_gt, n_nodes)\n",
        "    assert missing_gt.shape == (n_gt, n_nodes)\n",
        "    ks[np.expand_dims(missing_gt, axis=1)] = 0\n",
        "\n",
        "    # Compute the OKS.\n",
        "    n_visible_gt = np.sum(\n",
        "        (~missing_gt).astype(\"float64\"), axis=-1, keepdims=True\n",
        "    )  # (n_gt, 1)\n",
        "    oks = np.sum(ks, axis=-1) / n_visible_gt\n",
        "    assert oks.shape == (n_gt, n_pr)\n",
        "\n",
        "    return oks"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "U3MLxrqQRdfV"
      },
      "outputs": [],
      "source": [
        "def inference(model, device, X, xv, yv):\n",
        "  model = model.to(device)\n",
        "  _ = model.eval()\n",
        "  X = X.to(device)\n",
        "  with torch.no_grad():\n",
        "    y_preds = model(X)\n",
        "    peaks = find_peaks(y_preds.cpu().permute(0, 2, 3, 1).numpy(), xv.numpy(), yv.numpy())\n",
        "\n",
        "  return peaks\n",
        "\n",
        "def clip_peaks(peaks):\n",
        "  for p in peaks:\n",
        "    out_of_bounds = np.logical_not(np.logical_and(np.all(np.where(0 <= p, True, False), axis=1),\n",
        "                                                  np.all(np.where(p <= 160, True, False), axis=1)))\n",
        "    p[out_of_bounds, :] = np.nan\n",
        "\n",
        "  return peaks\n",
        "\n",
        "def get_match_scores(all_y_preds, all_y_gt, stddev=0.25):\n",
        "  match_scores = []\n",
        "  for i, j in zip(all_y_gt, all_y_preds):\n",
        "    oks = compute_oks(i, j, stddev=stddev)[0][0]\n",
        "    match_scores.append(oks)\n",
        "\n",
        "  return match_scores\n",
        "\n",
        "def evaluate(\n",
        "  match_scores,\n",
        "  num_positive_pairs,\n",
        "  num_false_negatives,\n",
        "  recall_thresholds,\n",
        "  match_score_thresholds,\n",
        "):\n",
        "\n",
        "  precisions = []\n",
        "  recalls = []\n",
        "\n",
        "  npig = num_positive_pairs + num_false_negatives  # total number of GT instances\n",
        "\n",
        "  for match_score_threshold in match_score_thresholds:\n",
        "\n",
        "      tp = np.cumsum(match_scores >= match_score_threshold)\n",
        "      fp = np.cumsum(match_scores < match_score_threshold)\n",
        "\n",
        "      rc = tp / npig\n",
        "      pr = tp / (fp + tp + np.spacing(1))\n",
        "\n",
        "      recall = rc[-1]  # best recall at this OKS threshold\n",
        "\n",
        "      # Ensure strictly decreasing precisions.\n",
        "      for i in range(len(pr) - 1, 0, -1):\n",
        "          if pr[i] > pr[i - 1]:\n",
        "              pr[i - 1] = pr[i]\n",
        "\n",
        "      # Find best precision at each recall threshold.\n",
        "      rc_inds = np.searchsorted(rc, recall_thresholds, side=\"left\")\n",
        "      precision = np.zeros(rc_inds.shape)\n",
        "      is_valid_rc_ind = rc_inds < len(pr)\n",
        "      precision[is_valid_rc_ind] = pr[rc_inds[is_valid_rc_ind]]\n",
        "\n",
        "      precisions.append(precision)\n",
        "      recalls.append(recall)\n",
        "\n",
        "  precisions = np.array(precisions)\n",
        "  recalls = np.array(recalls)\n",
        "\n",
        "  AP = precisions.mean(\n",
        "      axis=1\n",
        "  )  # AP = average precision over fixed set of recall thresholds\n",
        "  AR = recalls  # AR = max recall given a fixed number of detections per image\n",
        "\n",
        "  mAP = precisions.mean()  # mAP = mean over all OKS thresholds\n",
        "  mAR = recalls.mean()  # mAR = mean over all OKS thresholds\n",
        "\n",
        "  return precisions, AP, mAP, mAR"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jKbaoESL0jgx"
      },
      "source": [
        "## Evaluation Code"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "iIA4Sp94gr77"
      },
      "outputs": [],
      "source": [
        "# from matplotlib import pyplot as plt\n",
        "\n",
        "# fig, (ax1, ax2) = plt.subplots(1, 2, frameon=False)\n",
        "# ax1.imshow(X[0].cpu().permute(1, 2, 0).numpy(), cmap=\"gray\")\n",
        "# ax2.imshow(X[0].cpu().permute(1, 2, 0).numpy(), cmap=\"gray\")\n",
        "# ax1.scatter(all_y_preds[0][:, 0], all_y_preds[0][:, 1])\n",
        "# ax2.scatter(all_y_gt[0][:, 0], all_y_gt[0][:, 1])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "89r928d3WcwQ"
      },
      "outputs": [],
      "source": [
        "xv, yv = make_grid_vectors(\n",
        "    image_height=160,\n",
        "    image_width=160,\n",
        "    output_stride=2\n",
        ")\n",
        "\n",
        "# Get all y_preds and y_gt.\n",
        "all_y_preds, all_y_gt = [], []\n",
        "_ = model.eval()\n",
        "for x in val_dl:\n",
        "  X, gt_kp, _ = x\n",
        "  peaks = inference(model, device, X, xv, yv)\n",
        "  peaks = clip_peaks(peaks)\n",
        "\n",
        "  all_y_preds.append(peaks)\n",
        "\n",
        "  out_of_bounds = np.where(np.all(np.isclose(gt_kp.cpu().numpy(), 0), axis=1))[0].tolist()\n",
        "  gt_kp[out_of_bounds, :] = np.nan\n",
        "  all_y_gt.append(gt_kp)\n",
        "\n",
        "all_y_preds = np.concatenate(all_y_preds, axis=0)\n",
        "all_y_gt = np.concatenate(all_y_gt, axis=0)\n",
        "\n",
        "# Define thresholds.\n",
        "match_score_thresholds = np.linspace(0.5, 0.95, 10)\n",
        "recall_thresholds = np.linspace(0, 1, 101)\n",
        "\n",
        "# Get evaluation metrics.\n",
        "match_scores = get_match_scores(all_y_preds, all_y_gt)\n",
        "precisions, AP, mAP, mAR = evaluate(match_scores, 400, 0, recall_thresholds, match_score_thresholds)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "SxZ_rL0I_AHX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8ecdda07-dac2-41a4-d0cf-cc8d128fff33"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 0 | Loss: 0.01214\n",
            "Epoch: 0 | Loss: 0.00029\n",
            "Epoch: 0 | Loss: 0.00028\n",
            "Epoch: 0 | Loss: 0.00027\n",
            "Epoch: 0 | Loss: 0.00027\n",
            "Epoch: 0 | Loss: 0.00027\n",
            "Epoch: 0 | Loss: 0.00027\n",
            "Epoch: 0 | Loss: 0.00026\n",
            "TRAIN: --- 87.70135450363159s seconds ---\n",
            "VAL: --- 5.937970399856567s seconds ---\n",
            "Epoch: 1 | Loss: 0.00026\n",
            "Epoch: 1 | Loss: 0.00026\n",
            "Epoch: 1 | Loss: 0.00025\n",
            "Epoch: 1 | Loss: 0.00025\n",
            "Epoch: 1 | Loss: 0.00024\n",
            "Epoch: 1 | Loss: 0.00023\n",
            "Epoch: 1 | Loss: 0.00022\n",
            "Epoch: 1 | Loss: 0.00022\n",
            "TRAIN: --- 76.7921416759491s seconds ---\n",
            "VAL: --- 5.367237567901611s seconds ---\n",
            "Epoch: 2 | Loss: 0.00021\n",
            "Epoch: 2 | Loss: 0.00022\n",
            "Epoch: 2 | Loss: 0.00021\n",
            "Epoch: 2 | Loss: 0.00022\n",
            "Epoch: 2 | Loss: 0.00019\n",
            "Epoch: 2 | Loss: 0.00020\n",
            "Epoch: 2 | Loss: 0.00020\n",
            "Epoch: 2 | Loss: 0.00021\n",
            "TRAIN: --- 76.24320030212402s seconds ---\n",
            "VAL: --- 5.393160581588745s seconds ---\n",
            "Epoch: 0 | Loss: 0.02026\n",
            "Epoch: 0 | Loss: 0.00032\n",
            "Epoch: 0 | Loss: 0.00028\n",
            "Epoch: 0 | Loss: 0.00028\n",
            "Epoch: 0 | Loss: 0.00028\n",
            "Epoch: 0 | Loss: 0.00027\n",
            "Epoch: 0 | Loss: 0.00027\n",
            "Epoch: 0 | Loss: 0.00027\n",
            "TRAIN: --- 80.17505884170532s seconds ---\n",
            "VAL: --- 5.456448078155518s seconds ---\n",
            "Epoch: 1 | Loss: 0.00027\n",
            "Epoch: 1 | Loss: 0.00027\n",
            "Epoch: 1 | Loss: 0.00026\n",
            "Epoch: 1 | Loss: 0.00027\n",
            "Epoch: 1 | Loss: 0.00026\n",
            "Epoch: 1 | Loss: 0.00026\n",
            "Epoch: 1 | Loss: 0.00026\n",
            "Epoch: 1 | Loss: 0.00025\n",
            "TRAIN: --- 76.55715680122375s seconds ---\n",
            "VAL: --- 5.6024346351623535s seconds ---\n",
            "Epoch: 2 | Loss: 0.00026\n",
            "Epoch: 2 | Loss: 0.00024\n",
            "Epoch: 2 | Loss: 0.00024\n",
            "Epoch: 2 | Loss: 0.00025\n",
            "Epoch: 2 | Loss: 0.00024\n",
            "Epoch: 2 | Loss: 0.00023\n",
            "Epoch: 2 | Loss: 0.00023\n",
            "Epoch: 2 | Loss: 0.00023\n",
            "TRAIN: --- 78.8465268611908s seconds ---\n",
            "VAL: --- 8.489745378494263s seconds ---\n",
            "Epoch: 3 | Loss: 0.00023\n",
            "Epoch: 3 | Loss: 0.00023\n",
            "Epoch: 3 | Loss: 0.00023\n",
            "Epoch: 3 | Loss: 0.00021\n",
            "Epoch: 3 | Loss: 0.00022\n",
            "Epoch: 3 | Loss: 0.00022\n",
            "Epoch: 3 | Loss: 0.00021\n",
            "Epoch: 3 | Loss: 0.00021\n",
            "TRAIN: --- 83.20371341705322s seconds ---\n",
            "VAL: --- 6.30114483833313s seconds ---\n",
            "Epoch: 4 | Loss: 0.00021\n",
            "Epoch: 4 | Loss: 0.00019\n",
            "Epoch: 4 | Loss: 0.00019\n",
            "Epoch: 4 | Loss: 0.00024\n",
            "Epoch: 4 | Loss: 0.00020\n",
            "Epoch: 4 | Loss: 0.00020\n",
            "Epoch: 4 | Loss: 0.00018\n",
            "Epoch: 4 | Loss: 0.00020\n",
            "TRAIN: --- 78.44118118286133s seconds ---\n",
            "VAL: --- 5.607906818389893s seconds ---\n",
            "Epoch: 5 | Loss: 0.00018\n",
            "Epoch: 5 | Loss: 0.00018\n",
            "Epoch: 5 | Loss: 0.00018\n",
            "Epoch: 5 | Loss: 0.00020\n",
            "Epoch: 5 | Loss: 0.00016\n",
            "Epoch: 5 | Loss: 0.00016\n",
            "Epoch: 5 | Loss: 0.00016\n",
            "Epoch: 5 | Loss: 0.00017\n",
            "TRAIN: --- 78.55440163612366s seconds ---\n",
            "VAL: --- 7.394286870956421s seconds ---\n",
            "Epoch: 6 | Loss: 0.00015\n",
            "Epoch: 6 | Loss: 0.00018\n",
            "Epoch: 6 | Loss: 0.00016\n",
            "Epoch: 6 | Loss: 0.00015\n",
            "Epoch: 6 | Loss: 0.00014\n",
            "Epoch: 6 | Loss: 0.00016\n",
            "Epoch: 6 | Loss: 0.00012\n",
            "Epoch: 6 | Loss: 0.00016\n",
            "TRAIN: --- 83.03672337532043s seconds ---\n",
            "VAL: --- 7.611588478088379s seconds ---\n",
            "Epoch: 7 | Loss: 0.00015\n",
            "Epoch: 7 | Loss: 0.00012\n",
            "Epoch: 7 | Loss: 0.00012\n",
            "Epoch: 7 | Loss: 0.00013\n",
            "Epoch: 7 | Loss: 0.00012\n",
            "Epoch: 7 | Loss: 0.00017\n",
            "Epoch: 7 | Loss: 0.00013\n",
            "Epoch: 7 | Loss: 0.00011\n",
            "TRAIN: --- 78.20189595222473s seconds ---\n",
            "VAL: --- 5.601502895355225s seconds ---\n",
            "Epoch: 8 | Loss: 0.00012\n",
            "Epoch: 8 | Loss: 0.00011\n",
            "Epoch: 8 | Loss: 0.00011\n",
            "Epoch: 8 | Loss: 0.00011\n",
            "Epoch: 8 | Loss: 0.00014\n",
            "Epoch: 8 | Loss: 0.00011\n",
            "Epoch: 8 | Loss: 0.00012\n",
            "Epoch: 8 | Loss: 0.00009\n",
            "TRAIN: --- 82.37042260169983s seconds ---\n",
            "VAL: --- 6.13653564453125s seconds ---\n",
            "Epoch: 9 | Loss: 0.00010\n",
            "Epoch: 9 | Loss: 0.00010\n",
            "Epoch: 9 | Loss: 0.00018\n",
            "Epoch: 9 | Loss: 0.00013\n",
            "Epoch: 9 | Loss: 0.00007\n",
            "Epoch: 9 | Loss: 0.00009\n",
            "Epoch: 9 | Loss: 0.00010\n",
            "Epoch: 9 | Loss: 0.00010\n",
            "TRAIN: --- 77.73186898231506s seconds ---\n",
            "VAL: --- 8.432915687561035s seconds ---\n",
            "Epoch: 0 | Loss: 0.02578\n",
            "Epoch: 0 | Loss: 0.00030\n",
            "Epoch: 0 | Loss: 0.00028\n",
            "Epoch: 0 | Loss: 0.00027\n",
            "Epoch: 0 | Loss: 0.00027\n",
            "Epoch: 0 | Loss: 0.00027\n",
            "Epoch: 0 | Loss: 0.00027\n",
            "Epoch: 0 | Loss: 0.00027\n",
            "TRAIN: --- 78.4963767528534s seconds ---\n",
            "VAL: --- 5.586945295333862s seconds ---\n",
            "Epoch: 1 | Loss: 0.00026\n",
            "Epoch: 1 | Loss: 0.00027\n",
            "Epoch: 1 | Loss: 0.00027\n",
            "Epoch: 1 | Loss: 0.00027\n",
            "Epoch: 1 | Loss: 0.00026\n",
            "Epoch: 1 | Loss: 0.00026\n",
            "Epoch: 1 | Loss: 0.00026\n",
            "Epoch: 1 | Loss: 0.00025\n",
            "TRAIN: --- 83.0074474811554s seconds ---\n",
            "VAL: --- 9.312719106674194s seconds ---\n",
            "Epoch: 2 | Loss: 0.00024\n",
            "Epoch: 2 | Loss: 0.00024\n",
            "Epoch: 2 | Loss: 0.00024\n",
            "Epoch: 2 | Loss: 0.00023\n",
            "Epoch: 2 | Loss: 0.00023\n",
            "Epoch: 2 | Loss: 0.00022\n",
            "Epoch: 2 | Loss: 0.00023\n",
            "Epoch: 2 | Loss: 0.00022\n",
            "TRAIN: --- 77.89009428024292s seconds ---\n",
            "VAL: --- 6.171067714691162s seconds ---\n",
            "Epoch: 3 | Loss: 0.00021\n",
            "Epoch: 3 | Loss: 0.00021\n",
            "Epoch: 3 | Loss: 0.00022\n",
            "Epoch: 3 | Loss: 0.00021\n",
            "Epoch: 3 | Loss: 0.00021\n",
            "Epoch: 3 | Loss: 0.00020\n",
            "Epoch: 3 | Loss: 0.00020\n",
            "Epoch: 3 | Loss: 0.00020\n",
            "TRAIN: --- 78.21411466598511s seconds ---\n",
            "VAL: --- 5.548382997512817s seconds ---\n",
            "Epoch: 4 | Loss: 0.00019\n",
            "Epoch: 4 | Loss: 0.00019\n",
            "Epoch: 4 | Loss: 0.00019\n",
            "Epoch: 4 | Loss: 0.00018\n",
            "Epoch: 4 | Loss: 0.00019\n",
            "Epoch: 4 | Loss: 0.00016\n",
            "Epoch: 4 | Loss: 0.00017\n",
            "Epoch: 4 | Loss: 0.00017\n",
            "TRAIN: --- 82.20993065834045s seconds ---\n",
            "VAL: --- 8.304203033447266s seconds ---\n",
            "Epoch: 5 | Loss: 0.00016\n",
            "Epoch: 5 | Loss: 0.00015\n",
            "Epoch: 5 | Loss: 0.00016\n",
            "Epoch: 5 | Loss: 0.00017\n",
            "Epoch: 5 | Loss: 0.00016\n",
            "Epoch: 5 | Loss: 0.00017\n",
            "Epoch: 5 | Loss: 0.00015\n",
            "Epoch: 5 | Loss: 0.00016\n",
            "TRAIN: --- 74.81142067909241s seconds ---\n",
            "VAL: --- 8.571688890457153s seconds ---\n",
            "Epoch: 6 | Loss: 0.00016\n",
            "Epoch: 6 | Loss: 0.00014\n",
            "Epoch: 6 | Loss: 0.00013\n",
            "Epoch: 6 | Loss: 0.00014\n",
            "Epoch: 6 | Loss: 0.00017\n",
            "Epoch: 6 | Loss: 0.00014\n",
            "Epoch: 6 | Loss: 0.00015\n",
            "Epoch: 6 | Loss: 0.00011\n",
            "TRAIN: --- 77.50685381889343s seconds ---\n",
            "VAL: --- 7.323737621307373s seconds ---\n",
            "Epoch: 7 | Loss: 0.00012\n",
            "Epoch: 7 | Loss: 0.00011\n",
            "Epoch: 7 | Loss: 0.00011\n",
            "Epoch: 7 | Loss: 0.00009\n",
            "Epoch: 7 | Loss: 0.00014\n",
            "Epoch: 7 | Loss: 0.00015\n",
            "Epoch: 7 | Loss: 0.00010\n",
            "Epoch: 7 | Loss: 0.00014\n",
            "TRAIN: --- 76.32402491569519s seconds ---\n",
            "VAL: --- 5.509433031082153s seconds ---\n",
            "Epoch: 8 | Loss: 0.00013\n",
            "Epoch: 8 | Loss: 0.00009\n",
            "Epoch: 8 | Loss: 0.00008\n",
            "Epoch: 8 | Loss: 0.00008\n",
            "Epoch: 8 | Loss: 0.00008\n",
            "Epoch: 8 | Loss: 0.00011\n",
            "Epoch: 8 | Loss: 0.00012\n",
            "Epoch: 8 | Loss: 0.00010\n",
            "TRAIN: --- 76.02750492095947s seconds ---\n",
            "VAL: --- 5.434497356414795s seconds ---\n",
            "Epoch: 9 | Loss: 0.00008\n",
            "Epoch: 9 | Loss: 0.00012\n",
            "Epoch: 9 | Loss: 0.00006\n",
            "Epoch: 9 | Loss: 0.00011\n",
            "Epoch: 9 | Loss: 0.00008\n",
            "Epoch: 9 | Loss: 0.00006\n",
            "Epoch: 9 | Loss: 0.00009\n",
            "Epoch: 9 | Loss: 0.00009\n",
            "TRAIN: --- 79.90534090995789s seconds ---\n",
            "VAL: --- 7.693657398223877s seconds ---\n",
            "Epoch: 10 | Loss: 0.00011\n",
            "Epoch: 10 | Loss: 0.00010\n",
            "Epoch: 10 | Loss: 0.00014\n",
            "Epoch: 10 | Loss: 0.00010\n",
            "Epoch: 10 | Loss: 0.00010\n",
            "Epoch: 10 | Loss: 0.00008\n",
            "Epoch: 10 | Loss: 0.00010\n",
            "Epoch: 10 | Loss: 0.00009\n",
            "TRAIN: --- 74.44278335571289s seconds ---\n",
            "VAL: --- 9.299549341201782s seconds ---\n",
            "Epoch: 11 | Loss: 0.00007\n",
            "Epoch: 11 | Loss: 0.00009\n",
            "Epoch: 11 | Loss: 0.00008\n",
            "Epoch: 11 | Loss: 0.00009\n",
            "Epoch: 11 | Loss: 0.00009\n",
            "Epoch: 11 | Loss: 0.00009\n",
            "Epoch: 11 | Loss: 0.00007\n",
            "Epoch: 11 | Loss: 0.00010\n",
            "TRAIN: --- 75.5510516166687s seconds ---\n",
            "VAL: --- 6.525010585784912s seconds ---\n",
            "Epoch: 12 | Loss: 0.00012\n",
            "Epoch: 12 | Loss: 0.00007\n",
            "Epoch: 12 | Loss: 0.00008\n",
            "Epoch: 12 | Loss: 0.00011\n",
            "Epoch: 12 | Loss: 0.00008\n",
            "Epoch: 12 | Loss: 0.00008\n",
            "Epoch: 12 | Loss: 0.00008\n",
            "Epoch: 12 | Loss: 0.00010\n",
            "TRAIN: --- 79.94503951072693s seconds ---\n",
            "VAL: --- 5.57610821723938s seconds ---\n",
            "Epoch: 13 | Loss: 0.00007\n",
            "Epoch: 13 | Loss: 0.00008\n",
            "Epoch: 13 | Loss: 0.00008\n",
            "Epoch: 13 | Loss: 0.00011\n",
            "Epoch: 13 | Loss: 0.00013\n",
            "Epoch: 13 | Loss: 0.00007\n",
            "Epoch: 13 | Loss: 0.00007\n",
            "Epoch: 13 | Loss: 0.00006\n",
            "TRAIN: --- 75.89366602897644s seconds ---\n",
            "VAL: --- 5.609175205230713s seconds ---\n",
            "Epoch: 14 | Loss: 0.00008\n",
            "Epoch: 14 | Loss: 0.00009\n",
            "Epoch: 14 | Loss: 0.00012\n",
            "Epoch: 14 | Loss: 0.00008\n",
            "Epoch: 14 | Loss: 0.00009\n",
            "Epoch: 14 | Loss: 0.00005\n",
            "Epoch: 14 | Loss: 0.00009\n",
            "Epoch: 14 | Loss: 0.00007\n",
            "TRAIN: --- 75.37067198753357s seconds ---\n",
            "VAL: --- 5.931228160858154s seconds ---\n",
            "Epoch: 15 | Loss: 0.00005\n",
            "Epoch: 15 | Loss: 0.00007\n",
            "Epoch: 15 | Loss: 0.00010\n",
            "Epoch: 15 | Loss: 0.00009\n",
            "Epoch: 15 | Loss: 0.00009\n",
            "Epoch: 15 | Loss: 0.00006\n",
            "Epoch: 15 | Loss: 0.00006\n",
            "Epoch: 15 | Loss: 0.00013\n",
            "TRAIN: --- 78.88694953918457s seconds ---\n",
            "VAL: --- 8.118659973144531s seconds ---\n",
            "Epoch: 16 | Loss: 0.00007\n",
            "Epoch: 16 | Loss: 0.00011\n",
            "Epoch: 16 | Loss: 0.00014\n",
            "Epoch: 16 | Loss: 0.00007\n",
            "Epoch: 16 | Loss: 0.00008\n",
            "Epoch: 16 | Loss: 0.00008\n",
            "Epoch: 16 | Loss: 0.00007\n",
            "Epoch: 16 | Loss: 0.00008\n",
            "TRAIN: --- 73.05341053009033s seconds ---\n",
            "VAL: --- 9.357332468032837s seconds ---\n",
            "Epoch: 17 | Loss: 0.00008\n",
            "Epoch: 17 | Loss: 0.00006\n",
            "Epoch: 17 | Loss: 0.00007\n",
            "Epoch: 17 | Loss: 0.00008\n",
            "Epoch: 17 | Loss: 0.00006\n",
            "Epoch: 17 | Loss: 0.00008\n",
            "Epoch: 17 | Loss: 0.00011\n",
            "Epoch: 17 | Loss: 0.00007\n",
            "TRAIN: --- 74.15320324897766s seconds ---\n",
            "VAL: --- 10.320062637329102s seconds ---\n",
            "Epoch: 18 | Loss: 0.00006\n",
            "Epoch: 18 | Loss: 0.00006\n",
            "Epoch: 18 | Loss: 0.00012\n",
            "Epoch: 18 | Loss: 0.00004\n",
            "Epoch: 18 | Loss: 0.00012\n",
            "Epoch: 18 | Loss: 0.00007\n",
            "Epoch: 18 | Loss: 0.00008\n",
            "Epoch: 18 | Loss: 0.00009\n",
            "TRAIN: --- 76.47527599334717s seconds ---\n",
            "VAL: --- 6.020748615264893s seconds ---\n",
            "Epoch: 19 | Loss: 0.00009\n",
            "Epoch: 19 | Loss: 0.00006\n",
            "Epoch: 19 | Loss: 0.00006\n",
            "Epoch: 19 | Loss: 0.00005\n",
            "Epoch: 19 | Loss: 0.00012\n",
            "Epoch: 19 | Loss: 0.00008\n",
            "Epoch: 19 | Loss: 0.00006\n",
            "Epoch: 19 | Loss: 0.00005\n",
            "TRAIN: --- 76.16870951652527s seconds ---\n",
            "VAL: --- 5.519412279129028s seconds ---\n"
          ]
        }
      ],
      "source": [
        "filters = 32\n",
        "filters_rate = 1.5\n",
        "down_blocks = 4\n",
        "stem_blocks = 0\n",
        "up_blocks = 3\n",
        "\n",
        "all_AP, all_mAP, all_mAR = [], [], []\n",
        "\n",
        "for epochs in [3, 10, 20]:\n",
        "\n",
        "  train_ds = DataGenerator(labels)\n",
        "  train_dl = DataLoader(\n",
        "      train_ds,\n",
        "      batch_size=4,\n",
        "      shuffle=True,\n",
        "      num_workers=cores,\n",
        "      pin_memory=True,\n",
        "      drop_last=True,\n",
        "      prefetch_factor=2\n",
        "  )\n",
        "\n",
        "  val_ds = DataGenerator(val_labels, is_train=False)\n",
        "  val_dl = DataLoader(\n",
        "      val_ds,\n",
        "      batch_size=4,\n",
        "      shuffle=False,\n",
        "      num_workers=cores,\n",
        "      pin_memory=True,\n",
        "      drop_last=True,\n",
        "      prefetch_factor=2\n",
        "  )\n",
        "\n",
        "  unet = Unet(filters=filters,\n",
        "              filters_rate=filters_rate,\n",
        "              down_blocks=down_blocks,\n",
        "              stem_blocks=stem_blocks,\n",
        "              up_blocks=up_blocks)\n",
        "\n",
        "  in_channels = int(\n",
        "      filters\n",
        "      * (\n",
        "          filters_rate\n",
        "          ** (down_blocks + stem_blocks - 1 - up_blocks + 1)\n",
        "      )\n",
        "  )\n",
        "  model = nn.Sequential(*[\n",
        "      unet,\n",
        "      nn.Conv2d(in_channels=in_channels, out_channels=13, kernel_size=1, padding=\"same\")\n",
        "  ])\n",
        "\n",
        "  opt = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
        "  scheduler = torch.optim.lr_scheduler.OneCycleLR(opt, max_lr=0.01, steps_per_epoch=len(train_dl), epochs=epochs)\n",
        "\n",
        "  scaler = torch.cuda.amp.GradScaler()\n",
        "  model = model.to(device)\n",
        "\n",
        "  for epoch in range(epochs):\n",
        "      _ = model.train()\n",
        "      start_time = time.time()\n",
        "      train_loss = 0\n",
        "      for idx, batch in enumerate(train_dl):\n",
        "          X, _, y = batch\n",
        "          X = X.to(device)\n",
        "          y = y.to(device)\n",
        "\n",
        "          with torch.autocast(device):\n",
        "              y_preds = model(X)\n",
        "              loss = nn.MSELoss()(y_preds, y)\n",
        "          scaler.scale(loss).backward()\n",
        "          scaler.step(opt)\n",
        "          scaler.update()\n",
        "          opt.zero_grad()\n",
        "\n",
        "          if idx % 100 == 0:\n",
        "            print(f\"Epoch: {epoch} | Loss: {loss:.5f}\")\n",
        "\n",
        "          train_loss += loss\n",
        "\n",
        "      train_loss /= (idx+1)\n",
        "      train_time = time.time() - start_time\n",
        "      print(f\"TRAIN: --- {train_time}s seconds ---\")\n",
        "\n",
        "      _  = model.eval()\n",
        "      start_time = time.time()\n",
        "      val_loss = 0\n",
        "      for idx, batch in enumerate(val_dl):\n",
        "          X, _, y = batch\n",
        "          X = X.to(\"cuda\")\n",
        "          y = y.to(\"cuda\")\n",
        "\n",
        "          with torch.no_grad():\n",
        "              y_preds = model(X)\n",
        "              loss = nn.MSELoss()(y_preds, y)\n",
        "\n",
        "          val_loss += loss\n",
        "\n",
        "      val_loss /= (idx+1)\n",
        "      val_time = time.time() - start_time\n",
        "      print(f\"VAL: --- {val_time}s seconds ---\")\n",
        "\n",
        "      scheduler.step()\n",
        "\n",
        "  xv, yv = make_grid_vectors(\n",
        "      image_height=160,\n",
        "      image_width=160,\n",
        "      output_stride=2\n",
        "  )\n",
        "\n",
        "  # Get all y_preds and y_gt.\n",
        "  all_y_preds, all_y_gt = [], []\n",
        "  _ = model.eval()\n",
        "  for x in val_dl:\n",
        "    X, gt_kp, _ = x\n",
        "    peaks = inference(model, device, X, xv, yv)\n",
        "    peaks = clip_peaks(peaks)\n",
        "\n",
        "    all_y_preds.append(peaks)\n",
        "\n",
        "    out_of_bounds = np.where(np.all(np.isclose(gt_kp.cpu().numpy(), 0), axis=1))[0].tolist()\n",
        "    gt_kp[out_of_bounds, :] = np.nan\n",
        "    all_y_gt.append(gt_kp)\n",
        "\n",
        "  all_y_preds = np.concatenate(all_y_preds, axis=0)\n",
        "  all_y_gt = np.concatenate(all_y_gt, axis=0)\n",
        "\n",
        "  # Define thresholds.\n",
        "  match_score_thresholds = np.linspace(0.5, 0.95, 10)\n",
        "  recall_thresholds = np.linspace(0, 1, 101)\n",
        "\n",
        "  # Get evaluation metrics.\n",
        "  match_scores = get_match_scores(all_y_preds, all_y_gt)\n",
        "  precisions, AP, mAP, mAR = evaluate(match_scores, 400, 0, recall_thresholds, match_score_thresholds)\n",
        "\n",
        "  all_AP.append(AP)\n",
        "  all_mAP.append(mAP)\n",
        "  all_mAR.append(mAR)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "np.save(\"all_AP_pt.npy\", np.array(all_AP))\n",
        "np.save(\"all_mAP_pt.npy\", np.array(all_mAP))\n",
        "np.save(\"all_mAR_pt.npy\", np.array(all_mAR))"
      ],
      "metadata": {
        "id": "O9hy1asa_ZGl"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "WKKT2SqlS-19"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "BeeqrLbdupmE",
        "PWl3cG1_9WTK",
        "wSdTJYOdu4L6",
        "5lugn7a_HC0Q",
        "-vYsPusvviiu",
        "0yo95XzRKkba",
        "jzMa-PNPqxxA"
      ],
      "machine_shape": "hm",
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}